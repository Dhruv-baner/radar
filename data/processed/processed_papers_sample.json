[
  {
    "pdf_path": "../data/raw\\2601_06025v1.pdf",
    "title": "Manifold limit for the training of shallow graph convolutional neural networks",
    "page_count": 44,
    "char_count": 117139,
    "section_count": 4,
    "sections": {
      "Abstract": "Abstract\nWe study the discrete-to-continuum consistency of the training of shallow graph con-\nvolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a\nmanifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose\nlow-frequency spectrum approximates that of the Laplace-Beltrami operator of the under-\nlying smooth manifold, and shallow GCNNs of possibly infinite width are linear functionals\non the space of measures on the parameter space.",
      "Introduction": "1\nIntroduction\nAcross a variety of machine learning scenarios, it is common to assume that data points lie on\na smooth manifold, which is commonly referred to as the manifold assumption. One such sce-\nnario involves high-dimensional data where the intrinsic dimensionality is believed to be much\nlower. Another is when data is sampled on points of a physical surface in three-dimensional\nspace, as can be the case in applications of medical imaging, computer vision, and scientific\nmachine learning. ",
      "Discussion": "6\nDiscussion\n6.1\nOn the assumptions on the manifold\nIn this paper we imposed an assumption on the asymptotics of the spectral gaps of the manifold,\nnamely at most polynomial decay.\nHere, we want to illustrate the delicacy of spectral gap\nbehavior with respect to properties with no intuitive geometric description, such as arithmetic\nconsiderations for resonances.\nThe manifold assumption in geometric machine learning methods builds on possibly leveraging\ntwo aspects, a lower intrinsic dimension of",
      "References": "References\n[1] F. Bach. Breaking the curse of dimensionality with convex neural networks. Journal of\nMachine Learning Research, 18(19):1\u201353, 2017.\n[2] M. Belkin and P. Niyogi.\nConvergence of Laplacian eigenmaps.\nIn Proceedings of the\n20th International Conference on Neural Information Processing Systems (NeurIPS), page\n129\u2013136, Cambridge, MA, 2006. MIT Press.\n[3] J. Bourgain, H. Brezis, and P. Mironescu. Another look at Sobolev spaces. In Optimal\ncontrol and partial differential equations, pages"
    }
  },
  {
    "pdf_path": "../data/raw\\2601_06022v1.pdf",
    "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs",
    "page_count": 13,
    "char_count": 52133,
    "section_count": 5,
    "sections": {
      "Abstract": "Abstract\nLarge language models (LLMs) exhibit com-\nplementary strengths arising from differences\nin pretraining data, model architectures, and\ndecoding behaviors.\nInference-time ensem-\nbling provides a practical way to combine these\ncapabilities without retraining. However, ex-\nisting ensemble approaches suffer from fun-\ndamental limitations. Most rely on fixed fu-\nsion granularity, which lacks the flexibility re-\nquired for mid-generation adaptation and fails\nto adapt to different generation ch",
      "Introduction": "1\nIntroduction\nLarge language models (LLMs) have demonstrated\nstrong performance across a wide range of natural\nlanguage processing tasks (Hendrycks et al., 2020;\nCobbe et al., 2021; Ning et al.; Wei et al., 2025; Ai\net al., 2025; Zhang et al., 2025; Chen et al., 2024)\nand are increasingly deployed in real-world appli-\ncations (Grattafiori et al., 2024; OpenAI, 2024).\nHowever, model performance is not uniform across\ntasks: differences in pretraining data, model ar-\nchitectures, and decoding stra",
      "Experiments": "4\nExperiments\nIn this section, we conduct a series of experiments\nto systematically evaluate the effectiveness, and\ndesign decisions of ADAFUSE. Specifically, we\naim to answer the following research questions:\n\u2022 RQ1: Does ADAFUSE consistently enhance the\nperformance of base models across a wide range\nof tasks?\n\u2022 RQ2: How does the adaptive word-commitment\nmechanism in ADAFUSE affect the effectiveness\nof word-level ensemble decoding?\n\u2022 RQ3: How does incorporating diversity-aware\nensemble scaling i",
      "Conclusion": "5\nConclusion\nWe introduced ADAFUSE, an adaptive ensemble de-\ncoding framework that enables confidence-guided\nfusion of multiple large language models at infer-\nence time. Experiments across open-domain ques-\ntion answering, arithmetic reasoning, and machine\ntranslation show that ADAFUSE consistently out-\nperforms strong ensemble baselines under com-\nparable computational budgets. Further analyses\ndemonstrate the effectiveness of adaptive word\n\n--- PAGE 8 ---\ncommitment and controlled diversity i",
      "References": "References\nMengting Ai, Tianxin Wei, Yifan Chen, Zhichen Zeng,\nRitchie Zhao, Girish Varatkar, Bita Darvish Rouhani,\nXianfeng Tang, Hanghang Tong, and Jingrui He.\n2025. Resmoe: Space-efficient compression of mix-\nture of experts llms via residual restoration. arXiv\npreprint arXiv:2503.06881.\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen,\nKeyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi\nChen, Pei Chu, and 1 others. 2024. Internlm2 techni-\ncal report. arXiv preprint arXiv:2403.17297.\nLingjiao Chen,"
    }
  },
  {
    "pdf_path": "../data/raw\\2601_06021v1.pdf",
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "page_count": 21,
    "char_count": 74187,
    "section_count": 5,
    "sections": {
      "Abstract": "Abstract\nReinforcement learning (RL) has emerged as a\ncritical technique for enhancing LLM-based\ndeep search agents.\nHowever, existing ap-\nproaches primarily rely on binary outcome\nrewards, which fail to capture the compre-\nhensiveness and factuality of agents\u2019 reason-\ning process, and often lead to undesirable\nbehaviors such as shortcut exploitation and\nhallucinations. To address these limitations,\nwe propose Citation-aware Rubric Rewards\n(CaRR), a fine-grained reward framework for\ndeep search ",
      "Introduction": "1\nIntroduction\nRecently, LLM-based deep search agents have at-\ntracted growing attention for their ability to lever-\nage external web-browsing tools to solve complex,\nknowledge-intensive problems (Yao et al., 2023;\nWang et al., 2024; OpenAI, 2025a). A prominent\n*Work was done when JZ interned at Zhipu AI.\nThinking: \u2026\u2026\nAction: search(\u201cmid-2010s meteor event over Southeast\u201d)\nQuestion: The evidence for a prehistoric cosmic collision was found near a coastal \nsettlement planned by a Scottish enginee",
      "Experiments": "3\nExperiments\nIn this section, we conduct RL experiments to show\nthe effectiveness of context-aware rubric rewards\nand C-GRPO for training deep search agents.\n3.1\nExperiment Setup\nModels and Training Data.\nWe select Qwen3-\n4B-Thinking-2507 (Team, 2025b) and Qwen3-30B-\nA3B-Thinking-2507 (Team, 2025b) as our back-\nbone models, covering different model sizes and\narchitectures (dense and MoE). We use Deep-\nDive (Lu et al., 2025), an open-sourced deep search\ndataset, as our training data. This datase",
      "Conclusion": "5\nConclusion\nIn this work, we propose CaRR, a novel frame-\nwork that provides fine-grained rewards for deep\nsearch agents, taking into account reasoning com-\nprehensiveness, factual grounding, and evidence\nconnectivity. We further introduce C-GRPO, which\ncombines CaRR and outcome rewards in RL for\ntraining robust deep search agents. Our extensive\nexperiments demonstrate that C-GRPO achieves\nsignificant improvement over GRPO in both deep\nsearch benchmarks and open-ended research tasks.\n6\nLimitati",
      "References": "References\nRahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Pre-\nston Bowman, Joaquin Qui\u00f1onero Candela, Foivos\nTsimpourlas, Michael Sharman, Meghan Shah, An-\ndrea Vallone, Alex Beutel, Johannes Heidecke, and\nKaran Singhal. 2025. Healthbench: Evaluating large\nlanguage models towards improved human health.\nCoRR, abs/2505.08775.\nAkari Asai, Jacqueline He, Rulin Shao, Weijia Shi,\nAmanpreet Singh, Joseph Chee Chang, Kyle Lo,\nLuca Soldaini, Sergey Feldman, Mike D\u2019Arcy,\nDavid Wadden, Matt Latzke, Miny"
    }
  },
  {
    "pdf_path": "../data/raw\\2601_06016v1.pdf",
    "title": "LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection",
    "page_count": 14,
    "char_count": 56159,
    "section_count": 6,
    "sections": {
      "Abstract": "Abstract\nAutomated seizure detection from electroencephalography\n(EEG) remains difficult due to the large variability of seizure\ndynamics across patients, recording conditions, and clinical\nsettings. We introduce LookAroundNet, a transformer-based\nseizure detector that uses a wider temporal window of EEG\ndata to model seizure activity. The seizure detector incorpo-\nrates EEG signals before and after the segment of interest,\nreflecting how clinicians use surrounding context when inter-\npreting EE",
      "Introduction": "1\nIntroduction\nEpilepsy is a chronic brain disorder that causes unprovoked\nseizures due to temporary electrical disturbance in the brain.\nIt affects people of all ages worldwide and arises from var-\nious causes, including head trauma, infections, and genetic\nand environmental factors. However, in many cases, the un-\nderlying cause remains unknown, and symptoms often vary\nconsiderably between individuals. Epilepsy can greatly affect\nquality of life, but approximately three-quarters of those di-\na",
      "Related Work": "2\nRelated Work\nRecent research in automatic seizure detection increasingly\nfavors deep learning\u2013based approaches, particularly Convo-\nlutional Neural Networks (CNNs) and Transformer architec-\ntures.\nMost CNN-based studies employ a sliding-window\nstrategy, dividing continuous EEG recordings into shorter seg-\nments that are processed independently [27, 28, 29]. This al-\nlows CNNs to effectively capture spatial patterns and short-\nterm temporal dynamics. For instance, Thuwajit et al. [30]\nused dept",
      "Methods": "3\nMethods\n3.1\nData\nThe datasets used in this study are summarized in Ta-\nble 1, with additional details provided in Appendix A. These\ndatasets contain EEG recordings collected from multiple\nsources and captured in various environments, with record-\nings ranging from a few minutes to several days of monitor-\ning. All recordings were conducted using the standard 10-20\nelectrode system and include seizure annotations.\nTUSZ\nThe\nTemple\nUniversity\nHospital\nEEG\nSeizure\nCorpus\n(TUSZ) [44] is a dataset c",
      "Conclusions": "5\nConclusions\nOur evaluation shows that LookAroundNet delivers competi-\ntive, state-of-the-art performance on a range of out-of-sample\nEEG datasets. The model shows strong generalization across\ndiverse clinical settings, demographic groups, and both rou-\n7\n\n--- PAGE 7 ---\n0\n16\n32\n64\n128\nContext Window Duration (secs)\n20\n40\n60\n80\nF1-Score\na) TUSZ\n0\n16\n32\n64\n128\nContext Window Duration (secs)\nb) Kvikna\n0\n16\n32\n64\n128\nContext Window Duration (secs)\nc) Siena\n0\n16\n32\n64\n128\nContext Window Duration (s",
      "References": "References\n[1]\nJohn S Duncan et al. \u201cAdult epilepsy\u201d. In: The Lancet\n367.9516 (2006), pp. 1087\u20131100.\n[2]\nEvangelia Giourou et al. \u201cIntroduction to epilepsy and\nrelated brain disorders\u201d. In: Cyberphysical Systems for\nEpilepsy and Related Brain Disorders: Multi-parametric\nMonitoring and Analysis for Diagnosis and Optimal\nDisease Management (2015), pp. 11\u201338.\n[3]\nDieter Schmidt and Steven C Schachter. \u201cDrug treat-\nment of epilepsy in adults\u201d. In: Bmj 348 (2014).\n[4]\nRoland D Thijs et al. \u201cEpilepsy "
    }
  },
  {
    "pdf_path": "../data/raw\\2601_06009v1.pdf",
    "title": "Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem",
    "page_count": 20,
    "char_count": 32522,
    "section_count": 5,
    "sections": {
      "Abstract": "Abstract\nWe develop a practical framework for distinguishing diffusive stochastic processes from deterministic sig-\nnals using only a single discrete time series. Our approach is based on classical excursion and crossing\ntheorems for continuous semimartingales, which correlates number N\u03b5 of excursions of magnitude at least\n\u03b5 with the quadratic variation [X]T of the process. The scaling law holds universally for all continuous\nsemimartingales with finite quadratic variation, including general Ito",
      "Introduction": "1\nIntroduction\nThe problem of determining whether a discrete-time signal arises from deterministic chaos or stochastic\ndynamics has been studied for decades [1\u20133]. Earlier approaches, such as correlation dimension, Lyapunov\nexponents and classical surrogate tests, provided theoretical foundations but are now recognized as fragile\nunder short data, noise and nonstationarity [3, 4]. Modern work therefore emphasizes finite-sample, noise-\nrobust diagnostics operating directly on observed sequences [",
      "Methods": "3\nMethods\n3.1\nQuadratic Variation Estimation\nGiven discrete observations Xti, we estimate the quadratic variation and process-dependent theoretical\ncurve as\nd\n[X]T =\nn\u22121\nX\ni=0\n(Xti+1 \u2212Xti)2\nand\nNtheory\n\u03b5\n=\nd\n[X]T\n2\u03b52 .\nLikewise, given the signal, we can also compute the empirical value of the excursion count as Nemp\n\u03b5\n.\n3.2\nExcursion Invariant Statistics\nFor a variety of \u03b5, we compute K(\u03b5) = Nemp\n\u03b5\n/Ntheory\n\u03b5\n. A diffusion must satisfy K(\u03b5) \u22481 across scales. We\nchoose a range of \u03b5 where this is ",
      "Conclusion": "6\nConclusion\nWe introduced a principled excursion-asymptote law for continuous semimartingales of Ito type, and\ndemonstrated its use as a robust diagnostic to distinguish stochastic diffusion from deterministic/chaotic\ndynamics. Unlike many existing chaos-versus-noise discrimination methods that require extensive hyper-\nparameter tuning, multiple trained models, or subjective interpretations of surrogate metrics, our approach\nis fully self-contained and leverages a theoretically guaranteed scali",
      "References": "References\n[1]\nN. P. Subramaniyam, J. F. Donges, and J. Hyttinen. \u201cSignatures of chaotic and stochastic dynamics\nuncovered with epsilon-recurrence networks\u201d. In: Proceedings of the Royal Society A: Mathematical,\nPhysical and Engineering Sciences 471.2183 (Nov. 2015), p. 20150349. issn: 1471-2946. doi: 10.\n1098/rspa.2015.0349. url: http://dx.doi.org/10.1098/rspa.2015.0349.\n[2]\nD. M. Mateos, L. E. Riveaud, and P. W. Lamberti. \u201cDetecting dynamical changes in time series by\nusing the Jensen Shannon "
    }
  }
]