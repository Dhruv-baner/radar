{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fb887c",
   "metadata": {},
   "source": [
    "## **Notebook 03: LangGraph Agent Prototype**\n",
    "\n",
    "**Purpose**\n",
    "Build the core agentic workflow using LangGraph. This notebook establishes the multi-agent system that will power Radar's paper analysis pipeline.\n",
    "\n",
    "**What We'll Do**\n",
    "\n",
    "| Step | Task | Output |\n",
    "|------|------|--------|\n",
    "| 1 | **Setup LangGraph** | Import libraries and configure LLM client |\n",
    "| 2 | **Define Agent State** | Create state schema for data flow between agents |\n",
    "| 3 | **Build Paper Analyzer** | Agent that extracts key technical insights from papers |\n",
    "| 4 | **Build Simplifier** | Agent that generates accessible explanations |\n",
    "| 5 | **Build Industry Matcher** | Agent that maps research to use cases |\n",
    "| 6 | **Orchestrate Workflow** | Connect agents in LangGraph state machine |\n",
    "| 7 | **Test Pipeline** | Run complete workflow on sample papers |\n",
    "\n",
    "**Key Questions to Answer**\n",
    "- How do I structure state in LangGraph for multi-agent workflows?\n",
    "- What prompts effectively extract technical insights from papers?\n",
    "- How do I chain agents while maintaining context?\n",
    "- Can the pipeline handle multiple papers efficiently?\n",
    "\n",
    "**Expected Outcomes**\n",
    "- Working LangGraph state machine with 3+ agents\n",
    "- Tested prompts for paper analysis and simplification\n",
    "- Complete pipeline: Paper text -> Technical analysis -> Simple summary -> Industry applications\n",
    "- Performance baseline for optimization\n",
    "\n",
    "**Architecture**\n",
    "```\n",
    "Paper Text (from Notebook 02)\n",
    "    |\n",
    "    v\n",
    "Paper Analyzer Agent\n",
    "    |\n",
    "    v\n",
    "Simplifier Agent  \n",
    "    |\n",
    "    v\n",
    "Industry Matcher Agent\n",
    "    |\n",
    "    v\n",
    "Final Output (structured JSON)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5292b520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "\"\"\"\n",
    "Import LangGraph, LangChain, and Anthropic libraries.\n",
    "Configure Claude API client for agent operations.\n",
    "\"\"\"\n",
    "\n",
    "# Core libraries\n",
    "import json\n",
    "import os\n",
    "from typing import TypedDict, Annotated\n",
    "from datetime import datetime\n",
    "\n",
    "# LangGraph and LangChain\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09663770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n",
      "LLM model: claude-sonnet-4-20250514\n",
      "API key loaded: sk-ant-a...\n"
     ]
    }
   ],
   "source": [
    "# Verify API key\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
    "\n",
    "# Initialize Claude client\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"LLM model: claude-sonnet-4-20250514\")\n",
    "print(f\"API key loaded: {api_key[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84861372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED PROCESSED PAPERS\n",
      "================================================================================\n",
      "Papers available: 5\n",
      "\n",
      "1. Manifold limit for the training of shallow graph convolution...\n",
      "   Pages: 44 | Chars: 117,139\n",
      "   Sections: 4\n",
      "\n",
      "2. AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling f...\n",
      "   Pages: 13 | Chars: 52,133\n",
      "   Sections: 5\n",
      "\n",
      "3. Chaining the Evidence: Robust Reinforcement Learning for Dee...\n",
      "   Pages: 21 | Chars: 74,187\n",
      "   Sections: 5\n",
      "\n",
      "Ready to process with agents\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Sample Papers from Notebook 02\n",
    "\n",
    "\"\"\"\n",
    "Load the processed papers to use as input for our agents.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load processed data\n",
    "processed_path = '../data/processed/processed_papers_sample.json'\n",
    "\n",
    "with open(processed_path, 'r', encoding='utf-8') as f:\n",
    "    processed_papers = json.load(f)\n",
    "\n",
    "print(\"LOADED PROCESSED PAPERS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Papers available: {len(processed_papers)}\")\n",
    "\n",
    "for i, paper in enumerate(processed_papers[:3], 1):\n",
    "    print(f\"\\n{i}. {paper['title'][:60]}...\")\n",
    "    print(f\"   Pages: {paper['page_count']} | Chars: {paper['char_count']:,}\")\n",
    "    print(f\"   Sections: {paper['section_count']}\")\n",
    "\n",
    "print(\"\\nReady to process with agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d700fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State schema defined: AgentState\n",
      "\n",
      "State fields:\n",
      "  Input: paper_title, paper_text, paper_sections\n",
      "  Analyzer: technical_summary, key_methods, main_results, limitations\n",
      "  Simplifier: executive_summary, key_innovation, accessible_explanation\n",
      "  Meta: processing_stage, errors\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define LangGraph State\n",
    "\n",
    "\"\"\"\n",
    "Define the state structure that will flow between agents.\n",
    "Each agent reads from and writes to this shared state.\n",
    "\"\"\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State schema for our agent workflow.\n",
    "    Each agent can read and modify these fields.\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    paper_title: str\n",
    "    paper_text: str\n",
    "    paper_sections: dict\n",
    "    \n",
    "    # Paper Analyzer outputs\n",
    "    technical_summary: str\n",
    "    key_methods: str\n",
    "    main_results: str\n",
    "    limitations: str\n",
    "    \n",
    "    # Simplifier outputs\n",
    "    executive_summary: str\n",
    "    key_innovation: str\n",
    "    accessible_explanation: str\n",
    "    \n",
    "    # Metadata\n",
    "    processing_stage: str\n",
    "    errors: list\n",
    "\n",
    "print(\"State schema defined: AgentState\")\n",
    "print(\"\\nState fields:\")\n",
    "print(\"  Input: paper_title, paper_text, paper_sections\")\n",
    "print(\"  Analyzer: technical_summary, key_methods, main_results, limitations\")\n",
    "print(\"  Simplifier: executive_summary, key_innovation, accessible_explanation\")\n",
    "print(\"  Meta: processing_stage, errors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
