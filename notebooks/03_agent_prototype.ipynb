{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fb887c",
   "metadata": {},
   "source": [
    "## **Notebook 03: LangGraph Agent Prototype**\n",
    "\n",
    "**Purpose**\n",
    "Build the core agentic workflow using LangGraph. This notebook establishes the multi-agent system that will power Radar's paper analysis pipeline.\n",
    "\n",
    "**What We'll Do**\n",
    "\n",
    "| Step | Task | Output |\n",
    "|------|------|--------|\n",
    "| 1 | **Setup LangGraph** | Import libraries and configure LLM client |\n",
    "| 2 | **Define Agent State** | Create state schema for data flow between agents |\n",
    "| 3 | **Build Paper Analyzer** | Agent that extracts key technical insights from papers |\n",
    "| 4 | **Build Simplifier** | Agent that generates accessible explanations |\n",
    "| 5 | **Build Industry Matcher** | Agent that maps research to use cases |\n",
    "| 6 | **Orchestrate Workflow** | Connect agents in LangGraph state machine |\n",
    "| 7 | **Test Pipeline** | Run complete workflow on sample papers |\n",
    "\n",
    "**Key Questions to Answer**\n",
    "- How do I structure state in LangGraph for multi-agent workflows?\n",
    "- What prompts effectively extract technical insights from papers?\n",
    "- How do I chain agents while maintaining context?\n",
    "- Can the pipeline handle multiple papers efficiently?\n",
    "\n",
    "**Expected Outcomes**\n",
    "- Working LangGraph state machine with 3+ agents\n",
    "- Tested prompts for paper analysis and simplification\n",
    "- Complete pipeline: Paper text -> Technical analysis -> Simple summary -> Industry applications\n",
    "- Performance baseline for optimization\n",
    "\n",
    "**Architecture**\n",
    "```\n",
    "Paper Text (from Notebook 02)\n",
    "    |\n",
    "    v\n",
    "Paper Analyzer Agent\n",
    "    |\n",
    "    v\n",
    "Simplifier Agent  \n",
    "    |\n",
    "    v\n",
    "Industry Matcher Agent\n",
    "    |\n",
    "    v\n",
    "Final Output (structured JSON)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5292b520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "\"\"\"\n",
    "Import LangGraph, LangChain, and Anthropic libraries.\n",
    "Configure Claude API client for agent operations.\n",
    "\"\"\"\n",
    "\n",
    "# Core libraries\n",
    "import json\n",
    "import os\n",
    "from typing import TypedDict, Annotated\n",
    "from datetime import datetime\n",
    "\n",
    "# LangGraph and LangChain\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09663770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n",
      "LLM model: claude-sonnet-4-20250514\n",
      "API key loaded: sk-ant-a...\n"
     ]
    }
   ],
   "source": [
    "# Verify API key\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
    "\n",
    "# Initialize Claude client\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"LLM model: claude-sonnet-4-20250514\")\n",
    "print(f\"API key loaded: {api_key[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84861372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED PROCESSED PAPERS\n",
      "================================================================================\n",
      "Papers available: 5\n",
      "\n",
      "1. Manifold limit for the training of shallow graph convolution...\n",
      "   Pages: 44 | Chars: 117,139\n",
      "   Sections: 4\n",
      "\n",
      "2. AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling f...\n",
      "   Pages: 13 | Chars: 52,133\n",
      "   Sections: 5\n",
      "\n",
      "3. Chaining the Evidence: Robust Reinforcement Learning for Dee...\n",
      "   Pages: 21 | Chars: 74,187\n",
      "   Sections: 5\n",
      "\n",
      "Ready to process with agents\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Sample Papers from Notebook 02\n",
    "\n",
    "\"\"\"\n",
    "Load the processed papers to use as input for our agents.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load processed data\n",
    "processed_path = '../data/processed/processed_papers_sample.json'\n",
    "\n",
    "with open(processed_path, 'r', encoding='utf-8') as f:\n",
    "    processed_papers = json.load(f)\n",
    "\n",
    "print(\"LOADED PROCESSED PAPERS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Papers available: {len(processed_papers)}\")\n",
    "\n",
    "for i, paper in enumerate(processed_papers[:3], 1):\n",
    "    print(f\"\\n{i}. {paper['title'][:60]}...\")\n",
    "    print(f\"   Pages: {paper['page_count']} | Chars: {paper['char_count']:,}\")\n",
    "    print(f\"   Sections: {paper['section_count']}\")\n",
    "\n",
    "print(\"\\nReady to process with agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4690422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State schema defined with new structure:\n",
      "  Summary -> Challenge -> Solution -> Technical Points\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define LangGraph State (UPDATED)\n",
    "\n",
    "\"\"\"\n",
    "Define the state structure that will flow between agents.\n",
    "\"\"\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State schema for our agent workflow.\"\"\"\n",
    "    \n",
    "    # Input\n",
    "    paper_title: str\n",
    "    paper_text: str\n",
    "    paper_sections: dict\n",
    "    \n",
    "    # Paper Analyzer outputs\n",
    "    technical_summary: str\n",
    "    key_methods: str\n",
    "    main_results: str\n",
    "    limitations: str\n",
    "    \n",
    "    # Simplifier outputs (updated structure)\n",
    "    executive_summary: str          # Two-line summary\n",
    "    key_innovation: str              # Challenge bullets (reusing this field)\n",
    "    accessible_explanation: str      # Solution overview\n",
    "    technical_points: str            # NEW: Simplified technical points\n",
    "    \n",
    "    # Metadata\n",
    "    processing_stage: str\n",
    "    errors: list\n",
    "\n",
    "print(\"State schema defined with new structure:\")\n",
    "print(\"  Summary -> Challenge -> Solution -> Technical Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce536b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent defined: paper_analyzer_agent\n",
      "Extracts technical insights from research papers\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Paper Analyzer Agent\n",
    "\n",
    "\"\"\"\n",
    "First agent: Extracts technical insights from research papers.\n",
    "Identifies methods, results, and limitations.\n",
    "\"\"\"\n",
    "\n",
    "def paper_analyzer_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Analyze paper and extract technical insights.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state with paper_text\n",
    "    \n",
    "    Returns:\n",
    "        Updated state with technical analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert AI researcher analyzing academic papers. \n",
    "\n",
    "Paper Title: {state['paper_title']}\n",
    "\n",
    "Paper Content (excerpt):\n",
    "{state['paper_text'][:8000]}\n",
    "\n",
    "Your task: Extract the following technical insights:\n",
    "\n",
    "1. TECHNICAL SUMMARY (2-3 sentences): What problem does this solve and how?\n",
    "\n",
    "2. KEY METHODS (bullet points): What techniques/approaches were used?\n",
    "\n",
    "3. MAIN RESULTS (bullet points): What were the key findings or performance metrics?\n",
    "\n",
    "4. LIMITATIONS (bullet points): What are the acknowledged limitations or future work needed?\n",
    "\n",
    "Format your response as JSON:\n",
    "{{\n",
    "  \"technical_summary\": \"...\",\n",
    "  \"key_methods\": [\"...\", \"...\"],\n",
    "  \"main_results\": [\"...\", \"...\"],\n",
    "  \"limitations\": [\"...\", \"...\"]\n",
    "}}\n",
    "\n",
    "Be precise and technical. Use the language of the field.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        state['technical_summary'] = result['technical_summary']\n",
    "        state['key_methods'] = '\\n'.join(f\"- {m}\" for m in result['key_methods'])\n",
    "        state['main_results'] = '\\n'.join(f\"- {r}\" for r in result['main_results'])\n",
    "        state['limitations'] = '\\n'.join(f\"- {l}\" for l in result['limitations'])\n",
    "        state['processing_stage'] = 'analyzed'\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"Analyzer error: {str(e)}\")\n",
    "        state['processing_stage'] = 'analyzer_failed'\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"Agent defined: paper_analyzer_agent\")\n",
    "print(\"Extracts technical insights from research papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "366fdf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent defined: simplifier_agent\n",
      "Structure: Summary -> Challenge -> Solution -> Technical Points\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Simplifier Agent \n",
    "\n",
    "\"\"\"\n",
    "Second agent: Translates technical insights into accessible explanations.\n",
    "Uses a clear narrative structure: Summary -> Problem -> Solution -> Technical Details\n",
    "\"\"\"\n",
    "\n",
    "def simplifier_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Generate accessible explanations from technical analysis.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state with technical analysis\n",
    "    \n",
    "    Returns:\n",
    "        Updated state with simplified explanations\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert science communicator making AI research accessible to intelligent non-experts.\n",
    "\n",
    "Paper Title: {state['paper_title']}\n",
    "\n",
    "Technical Summary: {state['technical_summary']}\n",
    "\n",
    "Key Methods: {state['key_methods']}\n",
    "\n",
    "Main Results: {state['main_results']}\n",
    "\n",
    "Your task: Create a clear, scannable explanation using this structure:\n",
    "\n",
    "1. TWO-LINE SUMMARY: Hook the reader. What is this and why does it matter? Maximum 2 sentences.\n",
    "\n",
    "2. THE CHALLENGE (3-4 bullet points): What problem exists? What are researchers trying to solve? Set the context.\n",
    "\n",
    "3. WHAT THIS PAPER DOES (1 paragraph): Explain the approach/solution in simple terms. No jargon. Use analogies if helpful.\n",
    "\n",
    "4. KEY TECHNICAL POINTS (3-5 bullet points): Break down important technical aspects into simple language. Each bullet should be understandable without deep expertise.\n",
    "\n",
    "Format your response as JSON:\n",
    "{{\n",
    "  \"two_line_summary\": \"...\",\n",
    "  \"challenge_bullets\": [\"...\", \"...\", \"...\"],\n",
    "  \"solution_overview\": \"...\",\n",
    "  \"technical_points\": [\"...\", \"...\", \"...\"]\n",
    "}}\n",
    "\n",
    "Write clearly and concisely. Assume the reader is smart but not an AI researcher.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        state['executive_summary'] = result['two_line_summary']\n",
    "        state['key_innovation'] = '\\n'.join(f\"- {c}\" for c in result['challenge_bullets'])\n",
    "        state['accessible_explanation'] = result['solution_overview']\n",
    "        \n",
    "        # Store technical points separately\n",
    "        state['technical_points'] = '\\n'.join(f\"- {t}\" for t in result['technical_points'])\n",
    "        state['processing_stage'] = 'simplified'\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"Simplifier error: {str(e)}\")\n",
    "        state['processing_stage'] = 'simplifier_failed'\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"Agent defined: simplifier_agent\")\n",
    "print(\"Structure: Summary -> Challenge -> Solution -> Technical Points\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
