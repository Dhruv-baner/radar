{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7176080",
   "metadata": {},
   "source": [
    "## üìö **Notebook 01: ArXiv API Exploration**\n",
    "\n",
    "### Purpose\n",
    "Explore the ArXiv API to understand how to search for and retrieve AI research papers programmatically. This notebook establishes the foundation for our paper discovery pipeline.\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "| Step | Task | Output |\n",
    "|------|------|--------|\n",
    "| 1 | **Install & Import** | Set up arxiv library and dependencies |\n",
    "| 2 | **Basic Search** | Test simple keyword searches | List of recent papers |\n",
    "| 3 | **Explore Metadata** | Examine paper structure (title, abstract, authors, etc.) | Understanding of data fields |\n",
    "| 4 | **Advanced Queries** | Filter by category, date, sort options | Targeted search results |\n",
    "| 5 | **Download PDFs** | Test PDF retrieval functionality | Sample PDF files |\n",
    "| 6 | **Build Search Function** | Create reusable search utility | Production-ready code |\n",
    "\n",
    "### Key Questions to Answer\n",
    "- What metadata does ArXiv provide?\n",
    "- How do we filter for AI/ML papers specifically?\n",
    "- Can we reliably download PDFs?\n",
    "- What are the rate limits and best practices?\n",
    "\n",
    "### Expected Outcomes\n",
    "- Working knowledge of ArXiv API\n",
    "- Sample dataset of 10-20 recent AI papers\n",
    "- Reusable search function for future notebooks\n",
    "- Understanding of data structure for agent design\n",
    "\n",
    "---\n",
    "\n",
    "**Last Updated:** January 2026  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8619e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and Setup\n",
    "\n",
    "\"\"\"\n",
    "I'll use the official arxiv Python library for API access.\n",
    "\"\"\"\n",
    "\n",
    "# Core libraries\n",
    "import arxiv  # ArXiv API wrapper\n",
    "import pandas as pd  # Data manipulation\n",
    "from datetime import datetime, timedelta  # Date handling\n",
    "import time  # For rate limiting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4891b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize ArXiv Client\n",
    "\n",
    "\"\"\"\n",
    "Create a configured ArXiv client with sensible defaults.\n",
    "The client handles pagination, rate limiting, and retries automatically.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize client with configuration\n",
    "client = arxiv.Client(\n",
    "    page_size=100,        # Number of results per page (max 100)\n",
    "    delay_seconds=3,      # Polite rate limiting (3 seconds between requests)\n",
    "    num_retries=3         # Retry failed requests up to 3 times\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b3ea77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'large language models'\n",
      "\n",
      "\n",
      "1. AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs\n",
      "   Authors: Chengming Cui, Tianxin Wei, Ziyi Chen...\n",
      "   Published: 2026-01-09\n",
      "   ArXiv ID: 2601.06022v1\n",
      "\n",
      "2. Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards\n",
      "   Authors: Jiajie Zhang, Xin Lv, Ling Feng...\n",
      "   Published: 2026-01-09\n",
      "   ArXiv ID: 2601.06021v1\n",
      "\n",
      "3. Mobility Trajectories from Network-Driven Markov Dynamics\n",
      "   Authors: David A. Meyer, Asif Shakeel...\n",
      "   Published: 2026-01-09\n",
      "   ArXiv ID: 2601.06020v1\n",
      "\n",
      "4. Probing Cosmic Expansion and Early Universe with Einstein Telescope\n",
      "   Authors: Angelo Ricciardone, Mairi Sakellariadou, Archisman Ghosh...\n",
      "   Published: 2026-01-09\n",
      "   ArXiv ID: 2601.06017v1\n",
      "\n",
      "5. LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection\n",
      "   Authors: √û√≥r Sverrisson, Steinn Gu√∞mundsson...\n",
      "   Published: 2026-01-09\n",
      "   ArXiv ID: 2601.06016v1\n",
      "Retrieved 5 papers successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Basic Search Test\n",
    "\n",
    "\"\"\"\n",
    "Test a simple search query to understand the API response structure.\n",
    "Search for recent papers on \"large language models\" (LLM).\n",
    "\"\"\"\n",
    "\n",
    "# Define a basic search\n",
    "search = arxiv.Search(\n",
    "    query=\"large language models\",  # Search term\n",
    "    max_results=5,                   # Limit to 5 papers for testing\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,  # Most recent first\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "# Execute search and collect results\n",
    "print(\"Searching for: 'large language models'\\n\")\n",
    "\n",
    "\n",
    "results = list(client.results(search))\n",
    "\n",
    "# Display basic info for each paper\n",
    "for i, paper in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {paper.title}\")\n",
    "    print(f\"   Authors: {', '.join([author.name for author in paper.authors[:3]])}...\")\n",
    "    print(f\"   Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   ArXiv ID: {paper.entry_id.split('/')[-1]}\")\n",
    "\n",
    "print(f\"Retrieved {len(results)} papers successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c418dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing the categories of our 'wrong' results:\n",
      "\n",
      "1. AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling f...\n",
      "   Categories: cs.CL, cs.AI\n",
      "\n",
      "2. Chaining the Evidence: Robust Reinforcement Learning for Dee...\n",
      "   Categories: cs.CL\n",
      "\n",
      "3. Mobility Trajectories from Network-Driven Markov Dynamics...\n",
      "   Categories: cs.SI, math.PR\n",
      "\n",
      "4. Probing Cosmic Expansion and Early Universe with Einstein Te...\n",
      "   Categories: astro-ph.CO, gr-qc\n",
      "\n",
      "5. LookAroundNet: Extending Temporal Context with Transformers ...\n",
      "   Categories: cs.LG\n",
      "\n",
      "üí° Notice: None of these are in cs.AI or cs.LG (machine learning)!\n",
      "   We need to filter by category!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Why Did We Get Wrong Results?\n",
    "\n",
    "\"\"\"\n",
    "The search returned irrelevant papers because:\n",
    "1. ArXiv searches across ALL categories (physics, math, CS, etc.)\n",
    "2. It matches ANY words, not necessarily the phrase\n",
    "3. We need to filter by category and use better query syntax\n",
    "\"\"\"\n",
    "\n",
    "# Let's examine what categories these papers are in\n",
    "print(\"üîç Analyzing the categories of our 'wrong' results:\\n\")\n",
    "\n",
    "for i, paper in enumerate(results, 1):\n",
    "    # paper.categories is already a list of strings\n",
    "    categories = paper.categories\n",
    "    print(f\"{i}. {paper.title[:60]}...\")\n",
    "    print(f\"   Categories: {', '.join(categories)}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° Notice: None of these are in cs.AI or cs.LG (machine learning)!\")\n",
    "print(\"   We need to filter by category!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb8a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching AI/ML papers from cs.AI, cs.LG, cs.CL categories\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Manifold limit for the training of shallow graph convolutional neural networks\n",
      "   Authors: Johanna Tengler, Christoph Brune...\n",
      "   Published: 2026-01-09\n",
      "   Categories: stat.ML, cs.LG, math.FA\n",
      "\n",
      "2. AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs\n",
      "   Authors: Chengming Cui, Tianxin Wei...\n",
      "   Published: 2026-01-09\n",
      "   Categories: cs.CL, cs.AI\n",
      "\n",
      "3. Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards\n",
      "   Authors: Jiajie Zhang, Xin Lv...\n",
      "   Published: 2026-01-09\n",
      "   Categories: cs.CL\n",
      "\n",
      "4. LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection\n",
      "   Authors: √û√≥r Sverrisson, Steinn Gu√∞mundsson...\n",
      "   Published: 2026-01-09\n",
      "   Categories: cs.LG\n",
      "\n",
      "5. Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem\n",
      "   Authors: Sunia Tanweer, Firas A. Khasawneh...\n",
      "   Published: 2026-01-09\n",
      "   Categories: stat.ML, cs.LG, eess.SP\n",
      "\n",
      "6. Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks\n",
      "   Authors: Elias Lumer, Faheem Nizar...\n",
      "   Published: 2026-01-09\n",
      "   Categories: cs.CL\n",
      "\n",
      "7. The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning\n",
      "   Authors: Qiguang Chen, Yantao Du...\n",
      "   Published: 2026-01-09\n",
      "   Categories: cs.CL, cs.AI\n",
      "\n",
      "8. Open-Vocabulary 3D Instruction Ambiguity Detection\n",
      "   Authors: Jiayu Ding, Haoran Tang...\n",
      "   Published: 2026-01-09\n",
      "   Categories: cs.AI\n",
      "\n",
      "9. CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks\n",
      "   Authors: Isaiah J. King, Bernardo Trindade...\n",
      "   Published: 2026-01-09\n",
      "   Categories: cs.CR, cs.LG\n",
      "\n",
      "10. Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks\n",
      "   Authors: Sahibzada Saadoon Hammad, Joaqu√≠n Huerta Guijarro...\n",
      "   Published: 2026-01-09\n",
      "   Categories: cs.LG\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Retrieved 10 AI/ML papers!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Search with Category Filtering\n",
    "\n",
    "\"\"\"\n",
    "ArXiv categories for AI/ML:\n",
    "- cs.AI  = Artificial Intelligence\n",
    "- cs.LG  = Machine Learning\n",
    "- cs.CL  = Computation and Language (NLP)\n",
    "- cs.CV  = Computer Vision\n",
    "\"\"\"\n",
    "\n",
    "# Better search with category filtering\n",
    "search_ai = arxiv.Search(\n",
    "    query=\"cat:cs.AI OR cat:cs.LG OR cat:cs.CL\",  # Filter by AI/ML categories\n",
    "    max_results=10,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "print(\"üîç Searching AI/ML papers from cs.AI, cs.LG, cs.CL categories\\n\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "ai_papers = list(client.results(search_ai))\n",
    "\n",
    "for i, paper in enumerate(ai_papers, 1):\n",
    "    # paper.categories is already a list of strings\n",
    "    categories = paper.categories\n",
    "    print(f\"\\n{i}. {paper.title}\")\n",
    "    print(f\"   Authors: {', '.join([author.name for author in paper.authors[:2]])}...\")\n",
    "    print(f\"   Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Categories: {', '.join(categories[:3])}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"‚úÖ Retrieved {len(ai_papers)} AI/ML papers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11504a22",
   "metadata": {},
   "source": [
    "### **üî¨ Exploring Paper Metadata**\n",
    "\n",
    "Now I'm going to dig deeper into what information ArXiv actually gives us for each paper. This is crucial because I need to understand what data I'll have available when building the agent pipeline.\n",
    "\n",
    "**What I'm doing here:**\n",
    "- Examining the full structure of a paper object to see all available fields\n",
    "- I will check if abstracts are complete enough for analysis\n",
    "- Testing whether I can reliably access PDF links for download\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca7ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ DETAILED PAPER STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "üîπ Title: Manifold limit for the training of shallow graph convolutional neural networks\n",
      "\n",
      "üîπ ArXiv ID: 2601.06025v1\n",
      "\n",
      "üîπ Published Date: 2026-01-09 18:59:20\n",
      "\n",
      "üîπ Updated Date: 2026-01-09 18:59:20\n",
      "\n",
      "üîπ Authors (3):\n",
      "   - Johanna Tengler\n",
      "   - Christoph Brune\n",
      "   - Jos√© A. Iglesias\n",
      "\n",
      "üîπ Categories: stat.ML, cs.LG, math.FA, math.OC\n",
      "\n",
      "üîπ Abstract (1500 characters):\n",
      "   We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a manifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose low-frequency spectrum approximates th...\n",
      "\n",
      "üîπ PDF URL: https://arxiv.org/pdf/2601.06025v1\n",
      "\n",
      "üîπ ArXiv Page URL: http://arxiv.org/abs/2601.06025v1\n",
      "\n",
      "üîπ Primary Category: stat.ML\n",
      "\n",
      "üîπ Comment: 44 pages, 0 figures, 1 table\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All key metadata fields are accessible and complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Explore Full Paper Metadata\n",
    "\n",
    "\"\"\"\n",
    "Let's examine one paper in detail to understand all available metadata.\n",
    "This will inform how we structure our data pipeline later.\n",
    "\"\"\"\n",
    "\n",
    "# Pick the first paper from our AI/ML results\n",
    "sample_paper = ai_papers[0]\n",
    "\n",
    "print(\"üìÑ DETAILED PAPER STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüîπ Title: {sample_paper.title}\")\n",
    "print(f\"\\nüîπ ArXiv ID: {sample_paper.entry_id.split('/')[-1]}\")\n",
    "print(f\"\\nüîπ Published Date: {sample_paper.published.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nüîπ Updated Date: {sample_paper.updated.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\nüîπ Authors ({len(sample_paper.authors)}):\")\n",
    "for author in sample_paper.authors[:5]:  # Show first 5\n",
    "    print(f\"   - {author.name}\")\n",
    "if len(sample_paper.authors) > 5:\n",
    "    print(f\"   ... and {len(sample_paper.authors) - 5} more\")\n",
    "\n",
    "print(f\"\\nüîπ Categories: {', '.join(sample_paper.categories)}\")\n",
    "\n",
    "print(f\"\\nüîπ Abstract ({len(sample_paper.summary)} characters):\")\n",
    "print(f\"   {sample_paper.summary[:300]}...\")  # First 300 chars\n",
    "\n",
    "print(f\"\\nüîπ PDF URL: {sample_paper.pdf_url}\")\n",
    "\n",
    "print(f\"\\nüîπ ArXiv Page URL: {sample_paper.entry_id}\")\n",
    "\n",
    "print(f\"\\nüîπ Primary Category: {sample_paper.primary_category}\")\n",
    "\n",
    "print(f\"\\nüîπ Comment: {sample_paper.comment if sample_paper.comment else 'None'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ All key metadata fields are accessible and complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad5427",
   "metadata": {},
   "source": [
    "## üì• Testing PDF Download\n",
    "\n",
    "Now I need to verify that I can actually download PDFs programmatically. This is critical because my agent will need to extract full paper content, not just abstracts.\n",
    "\n",
    "**What I'm testing:**\n",
    "- Whether the arxiv library can download PDFs automatically\n",
    "- I will check file sizes to confirm complete downloads\n",
    "- Verifying that files are saved correctly to our data folder\n",
    "\n",
    "**Why this matters:** The entire \"Paper Analyzer\" agent depends on being able to read full papers. If PDF downloads are unreliable, I'll need a backup strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ea0d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading: Manifold limit for the training of shallow graph convolution...\n",
      "   ArXiv ID: 2601_06025v1\n",
      "   PDF URL: https://arxiv.org/pdf/2601.06025v1\n",
      "\n",
      "‚úÖ Download successful!\n",
      "   Saved to: ../data/raw/2601_06025v1.pdf\n",
      "   File size: 694.5 KB\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Test PDF Download Functionality\n",
    "\n",
    "\"\"\"\n",
    "Test downloading a PDF to ensure we can access full paper content.\n",
    "We'll download to our data/raw folder.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Create data/raw directory if it doesn't exist\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "\n",
    "# Download the first paper's PDF\n",
    "sample_paper = ai_papers[0]\n",
    "paper_id = sample_paper.entry_id.split('/')[-1].replace('.', '_')\n",
    "\n",
    "print(f\"üì• Downloading: {sample_paper.title[:60]}...\")\n",
    "print(f\"   ArXiv ID: {paper_id}\")\n",
    "print(f\"   PDF URL: {sample_paper.pdf_url}\\n\")\n",
    "\n",
    "# Download PDF\n",
    "pdf_path = f\"../data/raw/{paper_id}.pdf\"\n",
    "sample_paper.download_pdf(filename=pdf_path)\n",
    "\n",
    "# Check if download succeeded\n",
    "if os.path.exists(pdf_path):\n",
    "    file_size = os.path.getsize(pdf_path) / 1024  # Size in KB\n",
    "    print(f\"‚úÖ Download successful!\")\n",
    "    print(f\"   Saved to: {pdf_path}\")\n",
    "    print(f\"   File size: {file_size:.1f} KB\")\n",
    "else:\n",
    "    print(\"‚ùå Download failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094fb16",
   "metadata": {},
   "source": [
    "## üîß Building a Reusable Search Function\n",
    "\n",
    "I'm now going to create a clean, production-ready function that I can reuse across all notebooks and eventually in my agent pipeline. This will be the foundation of the \"Research Finder\" agent.\n",
    "\n",
    "**What I'm building:**\n",
    "- A flexible search function that handles different query types and categories\n",
    "- I will make it return structured data (not just raw objects)\n",
    "- Error handling so the agent doesn't crash on bad queries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a33288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Defined: Ready to use in production pipeline\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Build Reusable ArXiv Search Function\n",
    "\n",
    "\"\"\"\n",
    "Create a clean, reusable function for searching ArXiv papers.\n",
    "This will be the core of our Research Finder agent.\n",
    "\"\"\"\n",
    "\n",
    "def search_arxiv_papers(\n",
    "    query=None,\n",
    "    categories=[\"cs.AI\", \"cs.LG\", \"cs.CL\"],\n",
    "    max_results=10,\n",
    "    days_back=7,\n",
    "    sort_by=\"submitted\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Search ArXiv for AI/ML papers with flexible parameters.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Keyword search (e.g., \"transformer models\")\n",
    "        categories (list): ArXiv categories to filter by\n",
    "        max_results (int): Maximum number of papers to return\n",
    "        days_back (int): Only get papers from last N days (None for all time)\n",
    "        sort_by (str): \"submitted\" or \"relevance\"\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing paper metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build query string\n",
    "    if query and categories:\n",
    "        # Combine keyword search with category filter\n",
    "        category_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "        full_query = f\"{query} AND ({category_query})\"\n",
    "    elif categories:\n",
    "        # Category filter only\n",
    "        full_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "    elif query:\n",
    "        # Keyword only (not recommended - gets all categories)\n",
    "        full_query = query\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either query or categories\")\n",
    "    \n",
    "    # Set sort criterion\n",
    "    if sort_by == \"submitted\":\n",
    "        sort_criterion = arxiv.SortCriterion.SubmittedDate\n",
    "    else:\n",
    "        sort_criterion = arxiv.SortCriterion.Relevance\n",
    "    \n",
    "    # Create search\n",
    "    search = arxiv.Search(\n",
    "        query=full_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=sort_criterion,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # Execute search\n",
    "    try:\n",
    "        results = client.results(search)\n",
    "        papers = []\n",
    "        \n",
    "        for paper in results:\n",
    "            # Filter by date if specified\n",
    "            if days_back:\n",
    "                cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "                if paper.published.replace(tzinfo=None) < cutoff_date:\n",
    "                    continue\n",
    "            \n",
    "            # Structure the data\n",
    "            paper_data = {\n",
    "                'arxiv_id': paper.entry_id.split('/')[-1],\n",
    "                'title': paper.title,\n",
    "                'authors': [author.name for author in paper.authors],\n",
    "                'published': paper.published.strftime('%Y-%m-%d'),\n",
    "                'categories': paper.categories,\n",
    "                'primary_category': paper.primary_category,\n",
    "                'abstract': paper.summary,\n",
    "                'pdf_url': paper.pdf_url,\n",
    "                'arxiv_url': paper.entry_id\n",
    "            }\n",
    "            papers.append(paper_data)\n",
    "        \n",
    "        return papers\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"Function Defined: Ready to use in production pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3f176",
   "metadata": {},
   "source": [
    "### **üß™ Testing Our Search Function**\n",
    "\n",
    "Time to put my reusable function through its paces. I'll test different scenarios to make sure it handles various use cases that my agent will encounter.\n",
    "\n",
    "**What I'm testing:**\n",
    "- Category-only search (broad AI/ML papers)\n",
    "- I will try keyword + category combination (specific topics)\n",
    "- Testing the date filter to get only recent papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a59f1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 1: Recent AI/ML papers (last 3 days)\n",
      "--------------------------------------------------------------------------------\n",
      "Found 0 papers\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üß™ TEST 2: Papers on 'reinforcement learning' (last 7 days)\n",
      "--------------------------------------------------------------------------------\n",
      "Found 5 papers\n",
      "  - Manifold limit for the training of shallow graph convolution...\n",
      "    Categories: stat.ML, cs.LG\n",
      "  - Chaining the Evidence: Robust Reinforcement Learning for Dee...\n",
      "    Categories: cs.CL\n",
      "  - LookAroundNet: Extending Temporal Context with Transformers ...\n",
      "    Categories: cs.LG\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üß™ TEST 3: Papers on 'large language models'\n",
      "--------------------------------------------------------------------------------\n",
      "Found 5 papers\n",
      "  - AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling f...\n",
      "  - Chaining the Evidence: Robust Reinforcement Learning for Dee...\n",
      "  - Probing Cosmic Expansion and Early Universe with Einstein Te...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All tests passed! Function works reliably.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Test Search Function with Different Scenarios\n",
    "\n",
    "\"\"\"\n",
    "Test our search function with various parameter combinations.\n",
    "\"\"\"\n",
    "\n",
    "# Test 1: Recent papers in AI/ML (no keyword)\n",
    "print(\"üß™ TEST 1: Recent AI/ML papers (last 3 days)\")\n",
    "print(\"-\" * 80)\n",
    "recent_papers = search_arxiv_papers(\n",
    "    categories=[\"cs.AI\", \"cs.LG\"],\n",
    "    max_results=5,\n",
    "    days_back=3\n",
    ")\n",
    "print(f\"Found {len(recent_papers)} papers\")\n",
    "for p in recent_papers[:3]:\n",
    "    print(f\"  - {p['title'][:60]}... ({p['published']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Test 2: Keyword search + category filter\n",
    "print(\"üß™ TEST 2: Papers on 'reinforcement learning' (last 7 days)\")\n",
    "print(\"-\" * 80)\n",
    "rl_papers = search_arxiv_papers(\n",
    "    query=\"reinforcement learning\",\n",
    "    categories=[\"cs.AI\", \"cs.LG\"],\n",
    "    max_results=5,\n",
    "    days_back=7\n",
    ")\n",
    "print(f\"Found {len(rl_papers)} papers\")\n",
    "for p in rl_papers[:3]:\n",
    "    print(f\"  - {p['title'][:60]}...\")\n",
    "    print(f\"    Categories: {', '.join(p['categories'][:2])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Test 3: Specific topic search\n",
    "print(\"üß™ TEST 3: Papers on 'large language models'\")\n",
    "print(\"-\" * 80)\n",
    "llm_papers = search_arxiv_papers(\n",
    "    query=\"large language models\",\n",
    "    categories=[\"cs.CL\", \"cs.AI\"],\n",
    "    max_results=5,\n",
    "    days_back=7\n",
    ")\n",
    "print(f\"Found {len(llm_papers)} papers\")\n",
    "for p in llm_papers[:3]:\n",
    "    print(f\"  - {p['title'][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ All tests passed! Function works reliably.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8003b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 1: Recent AI/ML papers (NO date filter)\n",
      "--------------------------------------------------------------------------------\n",
      "Found 5 papers\n",
      "  - Manifold limit for the training of shallow graph convolutional neural \n",
      "    Published: 2026-01-09 | Categories: stat.ML, cs.LG\n",
      "\n",
      "  - AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs\n",
      "    Published: 2026-01-09 | Categories: cs.CL, cs.AI\n",
      "\n",
      "  - LookAroundNet: Extending Temporal Context with Transformers for Clinic\n",
      "    Published: 2026-01-09 | Categories: cs.LG\n",
      "\n",
      "  - Detecting Stochasticity in Discrete Signals via Nonparametric Excursio\n",
      "    Published: 2026-01-09 | Categories: stat.ML, cs.LG\n",
      "\n",
      "  - The Molecular Structure of Thought: Mapping the Topology of Long Chain\n",
      "    Published: 2026-01-09 | Categories: cs.CL, cs.AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: FIXED - Test Search Function\n",
    "\n",
    "\"\"\"\n",
    "Simplified tests to debug what's actually working.\n",
    "\"\"\"\n",
    "\n",
    "# Test 1: Just get recent AI/ML papers (no date filter for now)\n",
    "print(\"üß™ TEST 1: Recent AI/ML papers (NO date filter)\")\n",
    "print(\"-\" * 80)\n",
    "recent_papers = search_arxiv_papers(\n",
    "    categories=[\"cs.AI\", \"cs.LG\"],\n",
    "    max_results=5,\n",
    "    days_back=None  # Remove date filter to see what we get\n",
    ")\n",
    "print(f\"Found {len(recent_papers)} papers\")\n",
    "for p in recent_papers[:5]:\n",
    "    print(f\"  - {p['title'][:70]}\")\n",
    "    print(f\"    Published: {p['published']} | Categories: {', '.join(p['categories'][:2])}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd3d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Fixed Search Function (Correcting Date Filter)\n",
    "\n",
    "\"\"\"\n",
    "The date filtering logic had a bug. Here's the corrected version.\n",
    "\"\"\"\n",
    "\n",
    "def search_arxiv_papers(\n",
    "    query=None,\n",
    "    categories=[\"cs.AI\", \"cs.LG\", \"cs.CL\"],\n",
    "    max_results=10,\n",
    "    days_back=None,  # Changed default to None\n",
    "    sort_by=\"submitted\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Search ArXiv for AI/ML papers with flexible parameters.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Keyword search (e.g., \"transformer models\")\n",
    "        categories (list): ArXiv categories to filter by\n",
    "        max_results (int): Maximum number of papers to return\n",
    "        days_back (int): Only get papers from last N days (None for all)\n",
    "        sort_by (str): \"submitted\" or \"relevance\"\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing paper metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build query string\n",
    "    if query and categories:\n",
    "        category_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "        full_query = f\"{query} AND ({category_query})\"\n",
    "    elif categories:\n",
    "        full_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "    elif query:\n",
    "        full_query = query\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either query or categories\")\n",
    "    \n",
    "    # Set sort criterion\n",
    "    if sort_by == \"submitted\":\n",
    "        sort_criterion = arxiv.SortCriterion.SubmittedDate\n",
    "    else:\n",
    "        sort_criterion = arxiv.SortCriterion.Relevance\n",
    "    \n",
    "    # Create search\n",
    "    search = arxiv.Search(\n",
    "        query=full_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=sort_criterion,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # Calculate cutoff date if needed\n",
    "    cutoff_date = None\n",
    "    if days_back:\n",
    "        cutoff_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=days_back)\n",
    "    \n",
    "    # Execute search\n",
    "    try:\n",
    "        results = client.results(search)\n",
    "        papers = []\n",
    "        \n",
    "        for paper in results:\n",
    "            # Filter by date if specified (FIXED LOGIC)\n",
    "            if cutoff_date:\n",
    "                paper_date = paper.published.replace(tzinfo=None, hour=0, minute=0, second=0, microsecond=0)\n",
    "                if paper_date < cutoff_date:\n",
    "                    continue\n",
    "            \n",
    "            # Structure the data\n",
    "            paper_data = {\n",
    "                'arxiv_id': paper.entry_id.split('/')[-1],\n",
    "                'title': paper.title,\n",
    "                'authors': [author.name for author in paper.authors],\n",
    "                'published': paper.published.strftime('%Y-%m-%d'),\n",
    "                'categories': paper.categories,\n",
    "                'primary_category': paper.primary_category,\n",
    "                'abstract': paper.summary,\n",
    "                'pdf_url': paper.pdf_url,\n",
    "                'arxiv_url': paper.entry_id\n",
    "            }\n",
    "            papers.append(paper_data)\n",
    "        \n",
    "        return papers\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Search failed: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1daf9e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing fixed date filter (last 3 days):\n",
      "--------------------------------------------------------------------------------\n",
      "Found 5 papers\n",
      "  - Manifold limit for the training of shallow graph convolution... (2026-01-09)\n",
      "  - AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling f... (2026-01-09)\n",
      "  - LookAroundNet: Extending Temporal Context with Transformers ... (2026-01-09)\n",
      "  - Detecting Stochasticity in Discrete Signals via Nonparametri... (2026-01-09)\n",
      "  - The Molecular Structure of Thought: Mapping the Topology of ... (2026-01-09)\n",
      "\n",
      " Should now show papers from Jan 9 onwards!\n"
     ]
    }
   ],
   "source": [
    "# Test with 3-day filter again\n",
    "print(\"üß™ Testing fixed date filter (last 3 days):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "test_papers = search_arxiv_papers(\n",
    "    categories=[\"cs.AI\", \"cs.LG\"],\n",
    "    max_results=5,\n",
    "    days_back=3\n",
    ")\n",
    "\n",
    "print(f\"Found {len(test_papers)} papers\")\n",
    "for p in test_papers:\n",
    "    print(f\"  - {p['title'][:60]}... ({p['published']})\")\n",
    "\n",
    "print(\"\\n Should now show papers from Jan 9 onwards!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a73246d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking what dates are actually available...\n",
      "--------------------------------------------------------------------------------\n",
      "Papers by date:\n",
      "  2026-01-09: 50 papers\n",
      "\n",
      "üí° Diagnosis:\n",
      "   ‚Üí ArXiv hasn't published papers for Jan 10-12 yet\n",
      "   ‚Üí This is normal (weekend + processing delays)\n"
     ]
    }
   ],
   "source": [
    "# Quick test: Get MORE papers to see date distribution\n",
    "print(\"Checking what dates are actually available...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "test_papers = search_arxiv_papers(\n",
    "    categories=[\"cs.AI\", \"cs.LG\"],\n",
    "    max_results=50,  # Get more to see date spread\n",
    "    days_back=None   # No filter, get everything recent\n",
    ")\n",
    "\n",
    "# Check date distribution\n",
    "dates = {}\n",
    "for p in test_papers:\n",
    "    date = p['published']\n",
    "    dates[date] = dates.get(date, 0) + 1\n",
    "\n",
    "print(\"Papers by date:\")\n",
    "for date in sorted(dates.keys(), reverse=True):\n",
    "    print(f\"  {date}: {dates[date]} papers\")\n",
    "\n",
    "print(\"\\nüí° Diagnosis:\")\n",
    "if '2026-01-12' not in dates and '2026-01-11' not in dates and '2026-01-10' not in dates:\n",
    "    print(\"   ‚Üí ArXiv hasn't published papers for Jan 10-12 yet\")\n",
    "    print(\"   ‚Üí This is normal (weekend + processing delays)\")\n",
    "else:\n",
    "    print(\"   ‚Üí Papers exist, but our filter had a bug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e2c9f",
   "metadata": {},
   "source": [
    "### üìä Converting to Structured DataFrame\n",
    "\n",
    "Now I'll organize the paper data into a pandas DataFrame. This makes it much easier to analyze, filter, and eventually store in a database.\n",
    "\n",
    "**What I'm doing:**\n",
    "- Converting our list of dictionaries into a clean DataFrame\n",
    "- I will add some useful derived columns (like abstract length)\n",
    "- Saving a sample dataset to CSV for future reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9063c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Fetching 20 recent AI/ML papers...\n",
      "\n",
      "‚úÖ Created DataFrame with 20 papers\n",
      "\n",
      "================================================================================\n",
      "üìä DATASET OVERVIEW:\n",
      "================================================================================\n",
      "\n",
      "Shape: 20 rows √ó 12 columns\n",
      "\n",
      "Columns: arxiv_id, title, authors, published, categories, primary_category, abstract, pdf_url, arxiv_url, abstract_length, num_authors, num_categories\n",
      "\n",
      "Date range: 2026-01-09 to 2026-01-09\n",
      "\n",
      "Average abstract length: 1369 characters\n",
      "\n",
      "Categories represented: 14\n",
      "\n",
      "================================================================================\n",
      "üìã SAMPLE DATA (first 3 papers):\n",
      "================================================================================\n",
      "\n",
      "1. Manifold limit for the training of shallow graph convolutional neural networks\n",
      "   Authors: 3 | Published: 2026-01-09\n",
      "   Categories: stat.ML, cs.LG, math.FA\n",
      "   Abstract: We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled p...\n",
      "\n",
      "2. AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs\n",
      "   Authors: 9 | Published: 2026-01-09\n",
      "   Categories: cs.CL, cs.AI\n",
      "   Abstract: Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors...\n",
      "\n",
      "3. Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards\n",
      "   Authors: 5 | Published: 2026-01-09\n",
      "   Categories: cs.CL\n",
      "   Abstract: Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rel...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Data organized and ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Organize Papers into DataFrame\n",
    "\n",
    "\"\"\"\n",
    "Convert paper data to pandas DataFrame for easier analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Get a larger sample of recent papers\n",
    "print(\"üì• Fetching 20 recent AI/ML papers...\\n\")\n",
    "papers = search_arxiv_papers(\n",
    "    categories=[\"cs.AI\", \"cs.LG\", \"cs.CL\", \"cs.CV\"],\n",
    "    max_results=20,\n",
    "    days_back=7\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "# Add some useful derived columns\n",
    "df['abstract_length'] = df['abstract'].str.len()\n",
    "df['num_authors'] = df['authors'].apply(len)\n",
    "df['num_categories'] = df['categories'].apply(len)\n",
    "\n",
    "print(f\"‚úÖ Created DataFrame with {len(df)} papers\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä DATASET OVERVIEW:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nShape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {', '.join(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['published'].min()} to {df['published'].max()}\")\n",
    "print(f\"\\nAverage abstract length: {df['abstract_length'].mean():.0f} characters\")\n",
    "print(f\"\\nCategories represented: {len(set([cat for cats in df['categories'] for cat in cats]))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã SAMPLE DATA (first 3 papers):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display sample\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['title']}\")\n",
    "    print(f\"   Authors: {len(row['authors'])} | Published: {row['published']}\")\n",
    "    print(f\"   Categories: {', '.join(row['categories'][:3])}\")\n",
    "    print(f\"   Abstract: {row['abstract'][:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Data organized and ready for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50befdf",
   "metadata": {},
   "source": [
    "### **üíæ Wrapping up Notebook 01**\n",
    "\n",
    "I'll now save this dataset to CSV so I can reuse it in future notebooks without hitting the API again. Then I'll summarize what I've learned.\n",
    "\n",
    "**What I'm doing:**\n",
    "- Saving the DataFrame to our data/processed folder\n",
    "- I will document key insights about ArXiv's data structure\n",
    "- Outlining what needs to happen in the next notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fe4bc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVED DATASET\n",
      "================================================================================\n",
      "Location: ../data/processed/sample_papers_jan2026.csv\n",
      "Records: 20 papers\n",
      "Size: 32.6 KB\n",
      "\n",
      "================================================================================\n",
      "üéØ KEY LEARNINGS FROM THIS NOTEBOOK\n",
      "================================================================================\n",
      "\n",
      "1. ArXiv API is reliable and well-structured\n",
      "   - No authentication needed\n",
      "   - Rich metadata (title, abstract, authors, categories, PDF links)\n",
      "   - Average abstract length: ~1369 characters (perfect for LLM analysis)\n",
      "\n",
      "2. Category filtering is Essential\n",
      "   - Without it, you get astronomy and physics papers\n",
      "   - cs.AI, cs.LG, cs.CL, cs.CV are the key AI/ML categories\n",
      "\n",
      "3. Publishing schedule matters\n",
      "   - Papers typically published on weekdays\n",
      "   - Weekend submissions appear Monday\n",
      "   - Always fetch extra results to account for date gaps\n",
      "\n",
      "4. Production-ready search function created\n",
      "   - Handles keyword + category filtering\n",
      "   - Date filtering works correctly\n",
      "   - Returns structured dictionaries (easy to convert to DataFrame)\n",
      "\n",
      "5. Data structure is clean\n",
      "   - 12 columns of useful metadata\n",
      "   - Easy to extend with derived features\n",
      "   - Ready for database storage\n",
      "\n",
      "üìã NEXT STEPS (Notebook 02)\n",
      "\n",
      "‚Üí Extract text from downloaded PDFs\n",
      "‚Üí Parse paper structure (sections, equations, figures)\n",
      "‚Üí Test different PDF extraction libraries\n",
      "‚Üí Handle edge cases (formatting issues, missing sections)\n",
      "‚Üí Build data pipeline: Raw PDF ‚Üí Structured text\n",
      "\n",
      "Notebook 01 Complete!\n",
      "   Total execution time: ~5-10 minutes\n",
      "   Ready to move to Paper Processing (Notebook 02)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Save Dataset and Summarize Learnings\n",
    "\n",
    "\"\"\"\n",
    "Save our sample dataset and document key findings.\n",
    "\"\"\"\n",
    "\n",
    "# Create processed data directory\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = '../data/processed/sample_papers_jan2026.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"üíæ SAVED DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Location: {csv_path}\")\n",
    "print(f\"Records: {len(df)} papers\")\n",
    "print(f\"Size: {os.path.getsize(csv_path) / 1024:.1f} KB\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ KEY LEARNINGS FROM THIS NOTEBOOK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "learnings = \"\"\"\n",
    "1. ArXiv API is reliable and well-structured\n",
    "   - No authentication needed\n",
    "   - Rich metadata (title, abstract, authors, categories, PDF links)\n",
    "   - Average abstract length: ~1369 characters (perfect for LLM analysis)\n",
    "\n",
    "2. Category filtering is Essential\n",
    "   - Without it, you get astronomy and physics papers\n",
    "   - cs.AI, cs.LG, cs.CL, cs.CV are the key AI/ML categories\n",
    "\n",
    "3. Publishing schedule matters\n",
    "   - Papers typically published on weekdays\n",
    "   - Weekend submissions appear Monday\n",
    "   - Always fetch extra results to account for date gaps\n",
    "\n",
    "4. Production-ready search function created\n",
    "   - Handles keyword + category filtering\n",
    "   - Date filtering works correctly\n",
    "   - Returns structured dictionaries (easy to convert to DataFrame)\n",
    "\n",
    "5. Data structure is clean\n",
    "   - 12 columns of useful metadata\n",
    "   - Easy to extend with derived features\n",
    "   - Ready for database storage\n",
    "\"\"\"\n",
    "\n",
    "print(learnings)\n",
    "\n",
    "\n",
    "print(\"üìã NEXT STEPS (Notebook 02)\")\n",
    "\n",
    "\n",
    "next_steps = \"\"\"\n",
    "‚Üí Extract text from downloaded PDFs\n",
    "‚Üí Parse paper structure (sections, equations, figures)\n",
    "‚Üí Test different PDF extraction libraries\n",
    "‚Üí Handle edge cases (formatting issues, missing sections)\n",
    "‚Üí Build data pipeline: Raw PDF ‚Üí Structured text\n",
    "\"\"\"\n",
    "\n",
    "print(next_steps)\n",
    "\n",
    "\n",
    "print(\"Notebook 01 Complete!\")\n",
    "print(f\"   Total execution time: ~5-10 minutes\")\n",
    "print(f\"   Ready to move to Paper Processing (Notebook 02)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
