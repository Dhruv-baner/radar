{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7176080",
   "metadata": {},
   "source": [
    "## üìö **Notebook 01: ArXiv API Exploration**\n",
    "\n",
    "### Purpose\n",
    "Explore the ArXiv API to understand how to search for and retrieve AI research papers programmatically. This notebook establishes the foundation for our paper discovery pipeline.\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "| Step | Task | Output |\n",
    "|------|------|--------|\n",
    "| 1 | **Install & Import** | Set up arxiv library and dependencies |\n",
    "| 2 | **Basic Search** | Test simple keyword searches | List of recent papers |\n",
    "| 3 | **Explore Metadata** | Examine paper structure (title, abstract, authors, etc.) | Understanding of data fields |\n",
    "| 4 | **Advanced Queries** | Filter by category, date, sort options | Targeted search results |\n",
    "| 5 | **Download PDFs** | Test PDF retrieval functionality | Sample PDF files |\n",
    "| 6 | **Build Search Function** | Create reusable search utility | Production-ready code |\n",
    "\n",
    "### Key Questions to Answer\n",
    "- What metadata does ArXiv provide?\n",
    "- How do we filter for AI/ML papers specifically?\n",
    "- Can we reliably download PDFs?\n",
    "- What are the rate limits and best practices?\n",
    "\n",
    "### Expected Outcomes\n",
    "- Working knowledge of ArXiv API\n",
    "- Sample dataset of 10-20 recent AI papers\n",
    "- Reusable search function for future notebooks\n",
    "- Understanding of data structure for agent design\n",
    "\n",
    "---\n",
    "\n",
    "**Last Updated:** January 2026  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8619e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and Setup\n",
    "\n",
    "\"\"\"\n",
    "I'll use the official arxiv Python library for API access.\n",
    "\"\"\"\n",
    "\n",
    "# Core libraries\n",
    "import arxiv  # ArXiv API wrapper\n",
    "import pandas as pd  # Data manipulation\n",
    "from datetime import datetime, timedelta  # Date handling\n",
    "import time  # For rate limiting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4891b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize ArXiv Client\n",
    "\n",
    "\"\"\"\n",
    "Create a configured ArXiv client with sensible defaults.\n",
    "The client handles pagination, rate limiting, and retries automatically.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize client with configuration\n",
    "client = arxiv.Client(\n",
    "    page_size=100,        # Number of results per page (max 100)\n",
    "    delay_seconds=3,      # Polite rate limiting (3 seconds between requests)\n",
    "    num_retries=3         # Retry failed requests up to 3 times\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b3ea77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'large language models'\n",
      "\n",
      "\n",
      "1. Unveiling the 3D structure of the central molecular zone from stellar kinematics and photometry: The 50 and 20 km/s clouds\n",
      "   Authors: Francisco Nogueras-Lara, Ashley T. Barnes, Jonathan D. Henshaw...\n",
      "   Published: 2026-01-08\n",
      "   ArXiv ID: 2601.05252v1\n",
      "\n",
      "2. Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video\n",
      "   Authors: Zeren Jiang, Chuanxia Zheng, Iro Laina...\n",
      "   Published: 2026-01-08\n",
      "   ArXiv ID: 2601.05251v1\n",
      "\n",
      "3. QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer\n",
      "   Authors: Daniele Lizzio Bosco, Shuteng Wang, Giuseppe Serra...\n",
      "   Published: 2026-01-08\n",
      "   ArXiv ID: 2601.05250v1\n",
      "\n",
      "4. LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model\n",
      "   Authors: Zhuoyang Liu, Jiaming Liu, Hao Chen...\n",
      "   Published: 2026-01-08\n",
      "   ArXiv ID: 2601.05248v1\n",
      "\n",
      "5. Random Models and Guarded Logic\n",
      "   Authors: Oskar Fiuk...\n",
      "   Published: 2026-01-08\n",
      "   ArXiv ID: 2601.05247v1\n",
      "Retrieved 5 papers successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Basic Search Test\n",
    "\n",
    "\"\"\"\n",
    "Test a simple search query to understand the API response structure.\n",
    "Search for recent papers on \"large language models\" (LLM).\n",
    "\"\"\"\n",
    "\n",
    "# Define a basic search\n",
    "search = arxiv.Search(\n",
    "    query=\"large language models\",  # Search term\n",
    "    max_results=5,                   # Limit to 5 papers for testing\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,  # Most recent first\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "# Execute search and collect results\n",
    "print(\"Searching for: 'large language models'\\n\")\n",
    "\n",
    "\n",
    "results = list(client.results(search))\n",
    "\n",
    "# Display basic info for each paper\n",
    "for i, paper in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {paper.title}\")\n",
    "    print(f\"   Authors: {', '.join([author.name for author in paper.authors[:3]])}...\")\n",
    "    print(f\"   Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   ArXiv ID: {paper.entry_id.split('/')[-1]}\")\n",
    "\n",
    "print(f\"Retrieved {len(results)} papers successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c418dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing the categories of our 'wrong' results:\n",
      "\n",
      "1. Unveiling the 3D structure of the central molecular zone fro...\n",
      "   Categories: astro-ph.GA\n",
      "\n",
      "2. Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular V...\n",
      "   Categories: cs.CV\n",
      "\n",
      "3. QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quan...\n",
      "   Categories: cs.CV\n",
      "\n",
      "4. LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robo...\n",
      "   Categories: cs.RO\n",
      "\n",
      "5. Random Models and Guarded Logic...\n",
      "   Categories: cs.LO\n",
      "\n",
      "üí° Notice: None of these are in cs.AI or cs.LG (machine learning)!\n",
      "   We need to filter by category!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Why Did We Get Wrong Results?\n",
    "\n",
    "\"\"\"\n",
    "The search returned irrelevant papers because:\n",
    "1. ArXiv searches across ALL categories (physics, math, CS, etc.)\n",
    "2. It matches ANY words, not necessarily the phrase\n",
    "3. We need to filter by category and use better query syntax\n",
    "\"\"\"\n",
    "\n",
    "# Let's examine what categories these papers are in\n",
    "print(\"üîç Analyzing the categories of our 'wrong' results:\\n\")\n",
    "\n",
    "for i, paper in enumerate(results, 1):\n",
    "    # paper.categories is already a list of strings\n",
    "    categories = paper.categories\n",
    "    print(f\"{i}. {paper.title[:60]}...\")\n",
    "    print(f\"   Categories: {', '.join(categories)}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° Notice: None of these are in cs.AI or cs.LG (machine learning)!\")\n",
    "print(\"   We need to filter by category!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcb8a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching AI/ML papers from cs.AI, cs.LG, cs.CL categories\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Optimal Lower Bounds for Online Multicalibration\n",
      "   Authors: Natalie Collina, Jiuyao Lu...\n",
      "   Published: 2026-01-08\n",
      "   Categories: cs.LG, math.ST, stat.ML\n",
      "\n",
      "2. GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization\n",
      "   Authors: Shih-Yang Liu, Xin Dong...\n",
      "   Published: 2026-01-08\n",
      "   Categories: cs.CL, cs.AI, cs.LG\n",
      "\n",
      "3. RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation\n",
      "   Authors: Boyang Wang, Haoran Zhang...\n",
      "   Published: 2026-01-08\n",
      "   Categories: cs.CV, cs.AI, cs.RO\n",
      "\n",
      "4. Robust Reasoning as a Symmetry-Protected Topological Phase\n",
      "   Authors: Ilmo Sung...\n",
      "   Published: 2026-01-08\n",
      "   Categories: cs.LG, cond-mat.dis-nn, cs.AI\n",
      "\n",
      "5. Measuring and Fostering Peace through Machine Learning and Artificial Intelligence\n",
      "   Authors: P. Gilda, P. Dungarwal...\n",
      "   Published: 2026-01-08\n",
      "   Categories: cs.CL, cs.CY, cs.LG\n",
      "\n",
      "6. Learning Latent Action World Models In The Wild\n",
      "   Authors: Quentin Garrido, Tushar Nagarajan...\n",
      "   Published: 2026-01-08\n",
      "   Categories: cs.AI, cs.CV\n",
      "\n",
      "7. Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data\n",
      "   Authors: James Rice...\n",
      "   Published: 2026-01-08\n",
      "   Categories: stat.ML, cs.LG, econ.EM\n",
      "\n",
      "8. CAOS: Conformal Aggregation of One-Shot Predictors\n",
      "   Authors: Maja Waldron...\n",
      "   Published: 2026-01-08\n",
      "   Categories: stat.ML, cs.AI, cs.LG\n",
      "\n",
      "9. MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents\n",
      "   Authors: Tamil Sudaravan Mohan Doss, Michael Xu...\n",
      "   Published: 2026-01-08\n",
      "   Categories: cs.AI\n",
      "\n",
      "10. Internal Representations as Indicators of Hallucinations in Agent Tool Selection\n",
      "   Authors: Kait Healy, Bharathi Srinivasan...\n",
      "   Published: 2026-01-08\n",
      "   Categories: cs.AI\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Retrieved 10 AI/ML papers!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Search with Category Filtering\n",
    "\n",
    "\"\"\"\n",
    "ArXiv categories for AI/ML:\n",
    "- cs.AI  = Artificial Intelligence\n",
    "- cs.LG  = Machine Learning\n",
    "- cs.CL  = Computation and Language (NLP)\n",
    "- cs.CV  = Computer Vision\n",
    "\"\"\"\n",
    "\n",
    "# Better search with category filtering\n",
    "search_ai = arxiv.Search(\n",
    "    query=\"cat:cs.AI OR cat:cs.LG OR cat:cs.CL\",  # Filter by AI/ML categories\n",
    "    max_results=10,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "print(\"üîç Searching AI/ML papers from cs.AI, cs.LG, cs.CL categories\\n\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "ai_papers = list(client.results(search_ai))\n",
    "\n",
    "for i, paper in enumerate(ai_papers, 1):\n",
    "    # paper.categories is already a list of strings\n",
    "    categories = paper.categories\n",
    "    print(f\"\\n{i}. {paper.title}\")\n",
    "    print(f\"   Authors: {', '.join([author.name for author in paper.authors[:2]])}...\")\n",
    "    print(f\"   Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Categories: {', '.join(categories[:3])}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"‚úÖ Retrieved {len(ai_papers)} AI/ML papers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11504a22",
   "metadata": {},
   "source": [
    "### **üî¨ Exploring Paper Metadata**\n",
    "\n",
    "Now I'm going to dig deeper into what information ArXiv actually gives us for each paper. This is crucial because I need to understand what data I'll have available when building the agent pipeline.\n",
    "\n",
    "**What I'm doing here:**\n",
    "- Examining the full structure of a paper object to see all available fields\n",
    "- I will check if abstracts are complete enough for analysis\n",
    "- Testing whether I can reliably access PDF links for download\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca7ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ DETAILED PAPER STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "üîπ Title: Optimal Lower Bounds for Online Multicalibration\n",
      "\n",
      "üîπ ArXiv ID: 2601.05245v1\n",
      "\n",
      "üîπ Published Date: 2026-01-08 18:59:32\n",
      "\n",
      "üîπ Updated Date: 2026-01-08 18:59:32\n",
      "\n",
      "üîπ Authors (4):\n",
      "   - Natalie Collina\n",
      "   - Jiuyao Lu\n",
      "   - Georgy Noarov\n",
      "   - Aaron Roth\n",
      "\n",
      "üîπ Categories: cs.LG, math.ST, stat.ML\n",
      "\n",
      "üîπ Abstract (937 characters):\n",
      "   We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.\n",
      "  In the general setting where group functions can depend on both context and the learner's predictions, we prove an $Œ©(T^{2/3})$ lower bound on expected multicalibrat...\n",
      "\n",
      "üîπ PDF URL: https://arxiv.org/pdf/2601.05245v1\n",
      "\n",
      "üîπ ArXiv Page URL: http://arxiv.org/abs/2601.05245v1\n",
      "\n",
      "üîπ Primary Category: cs.LG\n",
      "\n",
      "üîπ Comment: None\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All key metadata fields are accessible and complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Explore Full Paper Metadata\n",
    "\n",
    "\"\"\"\n",
    "Let's examine one paper in detail to understand all available metadata.\n",
    "This will inform how we structure our data pipeline later.\n",
    "\"\"\"\n",
    "\n",
    "# Pick the first paper from our AI/ML results\n",
    "sample_paper = ai_papers[0]\n",
    "\n",
    "print(\"üìÑ DETAILED PAPER STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüîπ Title: {sample_paper.title}\")\n",
    "print(f\"\\nüîπ ArXiv ID: {sample_paper.entry_id.split('/')[-1]}\")\n",
    "print(f\"\\nüîπ Published Date: {sample_paper.published.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nüîπ Updated Date: {sample_paper.updated.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\nüîπ Authors ({len(sample_paper.authors)}):\")\n",
    "for author in sample_paper.authors[:5]:  # Show first 5\n",
    "    print(f\"   - {author.name}\")\n",
    "if len(sample_paper.authors) > 5:\n",
    "    print(f\"   ... and {len(sample_paper.authors) - 5} more\")\n",
    "\n",
    "print(f\"\\nüîπ Categories: {', '.join(sample_paper.categories)}\")\n",
    "\n",
    "print(f\"\\nüîπ Abstract ({len(sample_paper.summary)} characters):\")\n",
    "print(f\"   {sample_paper.summary[:300]}...\")  # First 300 chars\n",
    "\n",
    "print(f\"\\nüîπ PDF URL: {sample_paper.pdf_url}\")\n",
    "\n",
    "print(f\"\\nüîπ ArXiv Page URL: {sample_paper.entry_id}\")\n",
    "\n",
    "print(f\"\\nüîπ Primary Category: {sample_paper.primary_category}\")\n",
    "\n",
    "print(f\"\\nüîπ Comment: {sample_paper.comment if sample_paper.comment else 'None'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ All key metadata fields are accessible and complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad5427",
   "metadata": {},
   "source": [
    "## üì• Testing PDF Download\n",
    "\n",
    "Now I need to verify that I can actually download PDFs programmatically. This is critical because my agent will need to extract full paper content, not just abstracts.\n",
    "\n",
    "**What I'm testing:**\n",
    "- Whether the arxiv library can download PDFs automatically\n",
    "- I will check file sizes to confirm complete downloads\n",
    "- Verifying that files are saved correctly to our data folder\n",
    "\n",
    "**Why this matters:** The entire \"Paper Analyzer\" agent depends on being able to read full papers. If PDF downloads are unreliable, I'll need a backup strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ea0d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading: Optimal Lower Bounds for Online Multicalibration...\n",
      "   ArXiv ID: 2601_05245v1\n",
      "   PDF URL: https://arxiv.org/pdf/2601.05245v1\n",
      "\n",
      "‚úÖ Download successful!\n",
      "   Saved to: ../data/raw/2601_05245v1.pdf\n",
      "   File size: 684.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Test PDF Download Functionality\n",
    "\n",
    "\"\"\"\n",
    "Test downloading a PDF to ensure we can access full paper content.\n",
    "We'll download to our data/raw folder.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Create data/raw directory if it doesn't exist\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "\n",
    "# Download the first paper's PDF\n",
    "sample_paper = ai_papers[0]\n",
    "paper_id = sample_paper.entry_id.split('/')[-1].replace('.', '_')\n",
    "\n",
    "print(f\"üì• Downloading: {sample_paper.title[:60]}...\")\n",
    "print(f\"   ArXiv ID: {paper_id}\")\n",
    "print(f\"   PDF URL: {sample_paper.pdf_url}\\n\")\n",
    "\n",
    "# Download PDF\n",
    "pdf_path = f\"../data/raw/{paper_id}.pdf\"\n",
    "sample_paper.download_pdf(filename=pdf_path)\n",
    "\n",
    "# Check if download succeeded\n",
    "if os.path.exists(pdf_path):\n",
    "    file_size = os.path.getsize(pdf_path) / 1024  # Size in KB\n",
    "    print(f\"‚úÖ Download successful!\")\n",
    "    print(f\"   Saved to: {pdf_path}\")\n",
    "    print(f\"   File size: {file_size:.1f} KB\")\n",
    "else:\n",
    "    print(\"‚ùå Download failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094fb16",
   "metadata": {},
   "source": [
    "## üîß Building a Reusable Search Function\n",
    "\n",
    "I'm now going to create a clean, production-ready function that I can reuse across all notebooks and eventually in my agent pipeline. This will be the foundation of the \"Research Finder\" agent.\n",
    "\n",
    "**What I'm building:**\n",
    "- A flexible search function that handles different query types and categories\n",
    "- I will make it return structured data (not just raw objects)\n",
    "- Error handling so the agent doesn't crash on bad queries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a33288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Defined: Ready to use in production pipeline\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Build Reusable ArXiv Search Function\n",
    "\n",
    "\"\"\"\n",
    "Create a clean, reusable function for searching ArXiv papers.\n",
    "This will be the core of our Research Finder agent.\n",
    "\"\"\"\n",
    "\n",
    "def search_arxiv_papers(\n",
    "    query=None,\n",
    "    categories=[\"cs.AI\", \"cs.LG\", \"cs.CL\"],\n",
    "    max_results=10,\n",
    "    days_back=7,\n",
    "    sort_by=\"submitted\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Search ArXiv for AI/ML papers with flexible parameters.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Keyword search (e.g., \"transformer models\")\n",
    "        categories (list): ArXiv categories to filter by\n",
    "        max_results (int): Maximum number of papers to return\n",
    "        days_back (int): Only get papers from last N days (None for all time)\n",
    "        sort_by (str): \"submitted\" or \"relevance\"\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing paper metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build query string\n",
    "    if query and categories:\n",
    "        # Combine keyword search with category filter\n",
    "        category_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "        full_query = f\"{query} AND ({category_query})\"\n",
    "    elif categories:\n",
    "        # Category filter only\n",
    "        full_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "    elif query:\n",
    "        # Keyword only (not recommended - gets all categories)\n",
    "        full_query = query\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either query or categories\")\n",
    "    \n",
    "    # Set sort criterion\n",
    "    if sort_by == \"submitted\":\n",
    "        sort_criterion = arxiv.SortCriterion.SubmittedDate\n",
    "    else:\n",
    "        sort_criterion = arxiv.SortCriterion.Relevance\n",
    "    \n",
    "    # Create search\n",
    "    search = arxiv.Search(\n",
    "        query=full_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=sort_criterion,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # Execute search\n",
    "    try:\n",
    "        results = client.results(search)\n",
    "        papers = []\n",
    "        \n",
    "        for paper in results:\n",
    "            # Filter by date if specified\n",
    "            if days_back:\n",
    "                cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "                if paper.published.replace(tzinfo=None) < cutoff_date:\n",
    "                    continue\n",
    "            \n",
    "            # Structure the data\n",
    "            paper_data = {\n",
    "                'arxiv_id': paper.entry_id.split('/')[-1],\n",
    "                'title': paper.title,\n",
    "                'authors': [author.name for author in paper.authors],\n",
    "                'published': paper.published.strftime('%Y-%m-%d'),\n",
    "                'categories': paper.categories,\n",
    "                'primary_category': paper.primary_category,\n",
    "                'abstract': paper.summary,\n",
    "                'pdf_url': paper.pdf_url,\n",
    "                'arxiv_url': paper.entry_id\n",
    "            }\n",
    "            papers.append(paper_data)\n",
    "        \n",
    "        return papers\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"Function Defined: Ready to use in production pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
