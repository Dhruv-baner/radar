{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502b0cab",
   "metadata": {},
   "source": [
    "## ðŸ“„ Notebook 02: Paper Processing Pipeline\n",
    "\n",
    "### Purpose\n",
    "Build a robust pipeline to download PDFs from ArXiv and extract structured text. This is critical because my agents need to analyze full papers, not just abstracts.\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "| Step | Task | Output |\n",
    "|------|------|--------|\n",
    "| 1 | **Load Sample Data** | Reuse papers from Notebook 01 |\n",
    "| 2 | **Download PDFs** | Test batch PDF downloading | PDF files in data/raw |\n",
    "| 3 | **Extract Text** | Test PyMuPDF vs other libraries | Raw text extraction |\n",
    "| 4 | **Parse Structure** | Identify sections (Abstract, Methods, etc.) | Structured paper content |\n",
    "| 5 | **Handle Edge Cases** | Deal with equations, figures, formatting | Robust extraction |\n",
    "| 6 | **Build Pipeline Function** | Production-ready processing function | Reusable code |\n",
    "\n",
    "### Key Questions to Answer\n",
    "- What's the best library for PDF text extraction?\n",
    "- Can I reliably identify paper sections?\n",
    "- How do I handle equations and figures?\n",
    "- What error cases do I need to handle?\n",
    "\n",
    "### Expected Outcomes\n",
    "- Downloaded PDFs for 10-20 sample papers\n",
    "- Clean text extraction from PDFs\n",
    "- Section identification (Abstract, Introduction, Methods, Results, Conclusion)\n",
    "- Production function: `process_paper(pdf_path) -> structured_dict`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026e1d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDF version: 1.26.7\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and Setup\n",
    "\n",
    "\"\"\"\n",
    "Import libraries for PDF processing and text extraction.\n",
    "PyMuPDF (fitz) is the main library for PDF handling.\n",
    "\"\"\"\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import re\n",
    "\n",
    "# ArXiv library (for downloading)\n",
    "import arxiv\n",
    "\n",
    "# PDF processing\n",
    "import fitz  # PyMuPDF\n",
    "print(f\"PyMuPDF version: {fitz.__version__}\")\n",
    "\n",
    "# File management\n",
    "from datetime import datetime\n",
    "\n",
    "# Load our saved search function from Notebook 01\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba642b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ LOADED SAMPLE DATASET\n",
      "================================================================================\n",
      "Papers loaded: 20\n",
      "Columns: ['arxiv_id', 'title', 'authors', 'published', 'categories', 'primary_category', 'abstract', 'pdf_url', 'arxiv_url', 'abstract_length', 'num_authors', 'num_categories']\n",
      "\n",
      "First 5 papers:\n",
      "--------------------------------------------------------------------------------\n",
      "1. Manifold limit for the training of shallow graph convolution...\n",
      "   ArXiv ID: 2601.06025v1\n",
      "2. AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling f...\n",
      "   ArXiv ID: 2601.06022v1\n",
      "3. Chaining the Evidence: Robust Reinforcement Learning for Dee...\n",
      "   ArXiv ID: 2601.06021v1\n",
      "4. LookAroundNet: Extending Temporal Context with Transformers ...\n",
      "   ArXiv ID: 2601.06016v1\n",
      "5. Detecting Stochasticity in Discrete Signals via Nonparametri...\n",
      "   ArXiv ID: 2601.06009v1\n",
      "\n",
      "================================================================================\n",
      "âœ… Ready to download and process PDFs!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Our Sample Dataset\n",
    "\n",
    "\"\"\"\n",
    "Load the papers we collected in Notebook 01.\n",
    "We'll use these to test our PDF processing pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Load the saved CSV\n",
    "csv_path = '../data/processed/sample_papers_jan2026.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"ðŸ“‚ LOADED SAMPLE DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Papers loaded: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst 5 papers:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    print(f\"{idx+1}. {row['title'][:60]}...\")\n",
    "    print(f\"   ArXiv ID: {row['arxiv_id']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Ready to download and process PDFs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4747a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined: download_arxiv_pdf()\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Build PDF Download Function\n",
    "\n",
    "\"\"\"\n",
    "Create a function to download multiple PDFs from ArXiv.\n",
    "Includes error handling and progress tracking.\n",
    "\"\"\"\n",
    "\n",
    "def download_arxiv_pdf(arxiv_id, save_dir='../data/raw'):\n",
    "    \"\"\"\n",
    "    Download a single PDF from ArXiv.\n",
    "    \n",
    "    Args:\n",
    "        arxiv_id (str): ArXiv paper ID (e.g., '2601.05245v1')\n",
    "        save_dir (str): Directory to save PDFs\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to downloaded PDF, or None if failed\n",
    "    \"\"\"\n",
    "    # Create directory if needed\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Construct filename\n",
    "    safe_id = arxiv_id.replace('.', '_')\n",
    "    pdf_path = os.path.join(save_dir, f\"{safe_id}.pdf\")\n",
    "    \n",
    "    # Skip if already downloaded\n",
    "    if os.path.exists(pdf_path):\n",
    "        print(f\"SKIP: {arxiv_id} (already exists)\")\n",
    "        return pdf_path\n",
    "    \n",
    "    try:\n",
    "        # Search for the paper\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        client = arxiv.Client()\n",
    "        paper = next(client.results(search))\n",
    "        \n",
    "        # Download PDF\n",
    "        paper.download_pdf(filename=pdf_path)\n",
    "        \n",
    "        # Verify download\n",
    "        if os.path.exists(pdf_path):\n",
    "            size_kb = os.path.getsize(pdf_path) / 1024\n",
    "            print(f\"SUCCESS: {arxiv_id} ({size_kb:.1f} KB)\")\n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(f\"FAILED: {arxiv_id} (file not created)\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {arxiv_id} - {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Function defined: download_arxiv_pdf()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be371201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF DOWNLOAD PROGRESS\n",
      "================================================================================\n",
      "SKIP: 2601.06025v1 (already exists)\n",
      "SUCCESS: 2601.06022v1 (1805.0 KB)\n",
      "SUCCESS: 2601.06021v1 (5191.3 KB)\n",
      "SUCCESS: 2601.06016v1 (1446.9 KB)\n",
      "SUCCESS: 2601.06009v1 (11621.5 KB)\n",
      "SUCCESS: 2601.06007v1 (892.8 KB)\n",
      "SUCCESS: 2601.06002v1 (9278.3 KB)\n",
      "SUCCESS: 2601.05991v1 (2459.9 KB)\n",
      "SUCCESS: 2601.05988v1 (2712.2 KB)\n",
      "SUCCESS: 2601.05986v1 (402.2 KB)\n",
      "\n",
      "================================================================================\n",
      "DOWNLOAD SUMMARY\n",
      "================================================================================\n",
      "Successful: 10\n",
      "Failed: 0\n",
      "\n",
      "Ready for text extraction\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Download Sample PDFs\n",
    "\n",
    "\"\"\"\n",
    "Download PDFs for the first 10 papers from our dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Select first 10 papers\n",
    "sample_papers = df.head(10)\n",
    "\n",
    "print(\"PDF DOWNLOAD PROGRESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "downloaded_paths = []\n",
    "failed_ids = []\n",
    "\n",
    "for idx, row in sample_papers.iterrows():\n",
    "    arxiv_id = row['arxiv_id']\n",
    "    path = download_arxiv_pdf(arxiv_id)\n",
    "    \n",
    "    if path:\n",
    "        downloaded_paths.append(path)\n",
    "    else:\n",
    "        failed_ids.append(arxiv_id)\n",
    "    \n",
    "    # Polite rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOWNLOAD SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Successful: {len(downloaded_paths)}\")\n",
    "print(f\"Failed: {len(failed_ids)}\")\n",
    "\n",
    "if failed_ids:\n",
    "    print(f\"\\nFailed IDs: {', '.join(failed_ids)}\")\n",
    "\n",
    "print(\"\\nReady for text extraction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
