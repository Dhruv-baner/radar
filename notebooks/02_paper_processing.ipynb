{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502b0cab",
   "metadata": {},
   "source": [
    "## ðŸ“„ Notebook 02: Paper Processing Pipeline\n",
    "\n",
    "### Purpose\n",
    "Build a robust pipeline to download PDFs from ArXiv and extract structured text. This is critical because my agents need to analyze full papers, not just abstracts.\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "| Step | Task | Output |\n",
    "|------|------|--------|\n",
    "| 1 | **Load Sample Data** | Reuse papers from Notebook 01 |\n",
    "| 2 | **Download PDFs** | Test batch PDF downloading | PDF files in data/raw |\n",
    "| 3 | **Extract Text** | Test PyMuPDF vs other libraries | Raw text extraction |\n",
    "| 4 | **Parse Structure** | Identify sections (Abstract, Methods, etc.) | Structured paper content |\n",
    "| 5 | **Handle Edge Cases** | Deal with equations, figures, formatting | Robust extraction |\n",
    "| 6 | **Build Pipeline Function** | Production-ready processing function | Reusable code |\n",
    "\n",
    "### Key Questions to Answer\n",
    "- What's the best library for PDF text extraction?\n",
    "- Can I reliably identify paper sections?\n",
    "- How do I handle equations and figures?\n",
    "- What error cases do I need to handle?\n",
    "\n",
    "### Expected Outcomes\n",
    "- Downloaded PDFs for 10-20 sample papers\n",
    "- Clean text extraction from PDFs\n",
    "- Section identification (Abstract, Introduction, Methods, Results, Conclusion)\n",
    "- Production function: `process_paper(pdf_path) -> structured_dict`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026e1d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDF version: 1.26.7\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and Setup\n",
    "\n",
    "\"\"\"\n",
    "Import libraries for PDF processing and text extraction.\n",
    "PyMuPDF (fitz) is the main library for PDF handling.\n",
    "\"\"\"\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import re\n",
    "\n",
    "# ArXiv library (for downloading)\n",
    "import arxiv\n",
    "\n",
    "# PDF processing\n",
    "import fitz  # PyMuPDF\n",
    "print(f\"PyMuPDF version: {fitz.__version__}\")\n",
    "\n",
    "# File management\n",
    "from datetime import datetime\n",
    "\n",
    "# Load our saved search function from Notebook 01\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba642b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ LOADED SAMPLE DATASET\n",
      "================================================================================\n",
      "Papers loaded: 20\n",
      "Columns: ['arxiv_id', 'title', 'authors', 'published', 'categories', 'primary_category', 'abstract', 'pdf_url', 'arxiv_url', 'abstract_length', 'num_authors', 'num_categories']\n",
      "\n",
      "First 5 papers:\n",
      "--------------------------------------------------------------------------------\n",
      "1. Manifold limit for the training of shallow graph convolution...\n",
      "   ArXiv ID: 2601.06025v1\n",
      "2. AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling f...\n",
      "   ArXiv ID: 2601.06022v1\n",
      "3. Chaining the Evidence: Robust Reinforcement Learning for Dee...\n",
      "   ArXiv ID: 2601.06021v1\n",
      "4. LookAroundNet: Extending Temporal Context with Transformers ...\n",
      "   ArXiv ID: 2601.06016v1\n",
      "5. Detecting Stochasticity in Discrete Signals via Nonparametri...\n",
      "   ArXiv ID: 2601.06009v1\n",
      "\n",
      "================================================================================\n",
      "âœ… Ready to download and process PDFs!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Our Sample Dataset\n",
    "\n",
    "\"\"\"\n",
    "Load the papers we collected in Notebook 01.\n",
    "We'll use these to test our PDF processing pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Load the saved CSV\n",
    "csv_path = '../data/processed/sample_papers_jan2026.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"ðŸ“‚ LOADED SAMPLE DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Papers loaded: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst 5 papers:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    print(f\"{idx+1}. {row['title'][:60]}...\")\n",
    "    print(f\"   ArXiv ID: {row['arxiv_id']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Ready to download and process PDFs!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
