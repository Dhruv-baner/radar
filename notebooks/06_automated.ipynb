{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d7cc5e",
   "metadata": {},
   "source": [
    "## Notebook 06: Automated Weekly Digest\n",
    "\n",
    "### Purpose\n",
    "Build an automated system that discovers and analyzes AI research papers weekly without manual intervention. This transforms Radar from an on-demand tool into a continuous intelligence platform.\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "| Step | Task | Output |\n",
    "|------|------|--------|\n",
    "| 1 | **Build Digest Generator** | Python script that fetches and processes papers |\n",
    "| 2 | **Format Markdown Report** | Beautiful, scannable weekly digest document |\n",
    "| 3 | **Test Locally** | Generate sample digest for validation |\n",
    "| 4 | **Create GitHub Action** | Automated workflow definition |\n",
    "| 5 | **Configure Secrets** | Set up API keys for automation |\n",
    "| 6 | **Deploy and Schedule** | Set weekly trigger (every Monday) |\n",
    "\n",
    "### Key Questions to Answer\n",
    "- How do we select the most interesting papers from 50-200 weekly results?\n",
    "- What format makes the digest most useful and readable?\n",
    "- How do we handle API rate limits and costs in automation?\n",
    "- Where should the digest be stored (GitHub, email, website)?\n",
    "\n",
    "### Expected Outcomes\n",
    "- Python script generating weekly markdown reports\n",
    "- GitHub Actions workflow running every Monday at 9am\n",
    "- Automatic commits of digest reports to repository\n",
    "- Fully autonomous research monitoring with zero manual intervention\n",
    "\n",
    "### Automation Architecture\n",
    "```\n",
    "GitHub Actions Trigger (Weekly)\n",
    "    ↓\n",
    "Fetch Papers from ArXiv (last 7 days)\n",
    "    ↓\n",
    "Filter and Rank by Relevance\n",
    "    ↓\n",
    "Process Top 5-10 Papers Through Agents\n",
    "    ↓\n",
    "Generate Markdown Report\n",
    "    ↓\n",
    "Commit to reports/ Folder\n",
    "    ↓\n",
    "(Optional) Send Email Notification\n",
    "```\n",
    "\n",
    "### Design Philosophy\n",
    "\n",
    "**Efficiency over completeness:** Process 5-10 most relevant papers deeply rather than 100 papers superficially. Focus on quality insights that save time, not overwhelming information dumps.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c83261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n",
      "Reports directory: ..\\reports\n",
      "Ready to build digest generator\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and Setup\n",
    "\n",
    "\"\"\"\n",
    "Import libraries for automated digest generation.\n",
    "Reuses agent pipeline from previous notebooks.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# ArXiv and processing\n",
    "import arxiv\n",
    "import fitz\n",
    "\n",
    "# Agent pipeline\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from typing import TypedDict\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Claude\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found\")\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "# Create reports directory\n",
    "REPORTS_DIR = Path(\"../reports\")\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"Reports directory: {REPORTS_DIR}\")\n",
    "print(\"Ready to build digest generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0a50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent pipeline loaded\n",
      "Ready to process papers for digest\n"
     ]
    }
   ],
   "source": [
    "# Load Agent Pipeline\n",
    "\n",
    "\"\"\"\n",
    "Define agent state and functions for paper processing.\n",
    "Same pipeline as Notebooks 03 and 05.\n",
    "\"\"\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    paper_title: str\n",
    "    paper_text: str\n",
    "    paper_sections: dict\n",
    "    technical_summary: str\n",
    "    key_methods: str\n",
    "    main_results: str\n",
    "    limitations: str\n",
    "    executive_summary: str\n",
    "    key_innovation: str\n",
    "    accessible_explanation: str\n",
    "    technical_points: str\n",
    "    processing_stage: str\n",
    "    errors: list\n",
    "\n",
    "def paper_analyzer_agent(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"You are an expert AI researcher analyzing academic papers. \n",
    "\n",
    "Paper Title: {state['paper_title']}\n",
    "\n",
    "Paper Content (excerpt):\n",
    "{state['paper_text'][:8000]}\n",
    "\n",
    "Extract these technical insights:\n",
    "\n",
    "1. TECHNICAL SUMMARY (2-3 sentences): What problem does this solve and how?\n",
    "2. KEY METHODS (bullet points): What techniques/approaches were used?\n",
    "3. MAIN RESULTS (bullet points): Key findings or performance metrics?\n",
    "4. LIMITATIONS (bullet points): Acknowledged limitations or future work?\n",
    "\n",
    "Respond ONLY with valid JSON, no markdown:\n",
    "{{\n",
    "  \"technical_summary\": \"...\",\n",
    "  \"key_methods\": [\"...\", \"...\"],\n",
    "  \"main_results\": [\"...\", \"...\"],\n",
    "  \"limitations\": [\"...\", \"...\"]\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        content = response.content.strip()\n",
    "        \n",
    "        if content.startswith('```'):\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "        content = content.strip()\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        \n",
    "        state['technical_summary'] = result['technical_summary']\n",
    "        state['key_methods'] = '\\n'.join(f\"- {m}\" for m in result['key_methods'])\n",
    "        state['main_results'] = '\\n'.join(f\"- {r}\" for r in result['main_results'])\n",
    "        state['limitations'] = '\\n'.join(f\"- {l}\" for l in result['limitations'])\n",
    "        state['processing_stage'] = 'analyzed'\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"Analyzer error: {str(e)}\")\n",
    "        state['processing_stage'] = 'analyzer_failed'\n",
    "    \n",
    "    return state\n",
    "\n",
    "def simplifier_agent(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"You are an expert science communicator making AI research accessible.\n",
    "\n",
    "Paper Title: {state['paper_title']}\n",
    "Technical Summary: {state['technical_summary']}\n",
    "Key Methods: {state['key_methods']}\n",
    "Main Results: {state['main_results']}\n",
    "\n",
    "Create a clear explanation:\n",
    "\n",
    "1. TWO-LINE SUMMARY: What is this and why does it matter? Maximum 2 sentences.\n",
    "2. THE CHALLENGE (3-4 bullet points): What problem exists?\n",
    "3. WHAT THIS PAPER DOES (1 paragraph): Explain the approach in simple terms.\n",
    "4. KEY TECHNICAL POINTS (3-5 bullet points): Break down technical aspects simply.\n",
    "\n",
    "Respond ONLY with valid JSON, no markdown:\n",
    "{{\n",
    "  \"two_line_summary\": \"...\",\n",
    "  \"challenge_bullets\": [\"...\", \"...\", \"...\"],\n",
    "  \"solution_overview\": \"...\",\n",
    "  \"technical_points\": [\"...\", \"...\", \"...\"]\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        content = response.content.strip()\n",
    "        \n",
    "        if content.startswith('```'):\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "        content = content.strip()\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        \n",
    "        state['executive_summary'] = result['two_line_summary']\n",
    "        state['key_innovation'] = '\\n'.join(f\"- {c}\" for c in result['challenge_bullets'])\n",
    "        state['accessible_explanation'] = result['solution_overview']\n",
    "        state['technical_points'] = '\\n'.join(f\"- {t}\" for t in result['technical_points'])\n",
    "        state['processing_stage'] = 'simplified'\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"Simplifier error: {str(e)}\")\n",
    "        state['processing_stage'] = 'simplifier_failed'\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"analyzer\", paper_analyzer_agent)\n",
    "workflow.add_node(\"simplifier\", simplifier_agent)\n",
    "workflow.set_entry_point(\"analyzer\")\n",
    "workflow.add_edge(\"analyzer\", \"simplifier\")\n",
    "workflow.add_edge(\"simplifier\", END)\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Agent pipeline loaded\")\n",
    "print(\"Ready to process papers for digest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd704b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching papers from 2026-01-21 to 2026-01-28\n",
      "Found 20 papers in date range\n",
      "\n",
      "Sample papers:\n",
      "1. Evaluation of Oncotimia: An LLM based system for supporting ...\n",
      "   Published: 2026-01-27\n",
      "2. DuwatBench: Bridging Language and Visual Heritage through an...\n",
      "   Published: 2026-01-27\n",
      "3. Self-Distillation Enables Continual Learning...\n",
      "   Published: 2026-01-27\n"
     ]
    }
   ],
   "source": [
    "# Fetch Papers from Last Week\n",
    "\n",
    "\"\"\"\n",
    "Search ArXiv for papers published in the last 7 days.\n",
    "Filters by AI/ML categories and sorts by date.\n",
    "\"\"\"\n",
    "\n",
    "def fetch_weekly_papers(days_back=7, max_papers=50):\n",
    "    \"\"\"\n",
    "    Fetch recent papers from ArXiv.\n",
    "    \n",
    "    Args:\n",
    "        days_back (int): Number of days to look back\n",
    "        max_papers (int): Maximum papers to fetch\n",
    "    \n",
    "    Returns:\n",
    "        list: Paper metadata dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days_back)\n",
    "    \n",
    "    print(f\"Fetching papers from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Search ArXiv\n",
    "    categories = [\"cs.AI\", \"cs.LG\", \"cs.CL\", \"cs.CV\"]\n",
    "    query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_papers,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "    \n",
    "    for paper in client.results(search):\n",
    "        # Filter by date\n",
    "        paper_date = paper.published.replace(tzinfo=None)\n",
    "        if paper_date < start_date:\n",
    "            continue\n",
    "        \n",
    "        papers.append({\n",
    "            'arxiv_id': paper.entry_id.split('/')[-1],\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'published': paper.published.strftime('%Y-%m-%d'),\n",
    "            'categories': paper.categories,\n",
    "            'abstract': paper.summary,\n",
    "            'pdf_url': paper.pdf_url\n",
    "        })\n",
    "    \n",
    "    print(f\"Found {len(papers)} papers in date range\")\n",
    "    return papers\n",
    "\n",
    "# Test the function\n",
    "test_papers = fetch_weekly_papers(days_back=7, max_papers=20)\n",
    "\n",
    "print(f\"\\nSample papers:\")\n",
    "for i, paper in enumerate(test_papers[:3], 1):\n",
    "    print(f\"{i}. {paper['title'][:60]}...\")\n",
    "    print(f\"   Published: {paper['published']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
